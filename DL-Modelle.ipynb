{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensor Based Activity Recoginition \n",
    "Challenge: cdl1 - Sensor based Activity Recognition  \n",
    "Team: Lea Bütler, Manjavy Kirupa, Etienne Roulet, Si Ben Tran  \n",
    "\n",
    "Aufgabe: DL Modell erstellen\n",
    "\n",
    "Hier in diesem Notebook erstellen wir unsere Deep Learning Modelle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Importieren & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn Libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn.model_selection import KFold, train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# keras Libraries\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Tensorflow Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# dataclass\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# general setup\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "# datetime as filename for logging\n",
    "now = datetime.now()\n",
    "date_time_string = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, filename=f\"{date_time_string}.txt\", filemode=\"a\"\n",
    ")\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "# Num of Gpus\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Parameters\n",
    "@dataclass\n",
    "class Parameters:\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 15\n",
    "    verbosity: str = \"auto\"\n",
    "    number_folds: int = 2\n",
    "    output_size: int = 6\n",
    "    window_size: int = 300\n",
    "    step_size: int = 100\n",
    "\n",
    "# Getrimmte Sensordaten einlesen\n",
    "df = pd.read_csv(\"../Sensor_Data-Wrangling-und-EDA/Alle_Messungen_trimmed.csv\", index_col=0)\n",
    "display(df)\n",
    "\n",
    "# convert the string time column to datetime\n",
    "epoch = pd.Timestamp(\"1970-01-01\")\n",
    "\n",
    "# Time column into datetime\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "\n",
    "# Time column into milliseconds\n",
    "df[\"time\"] = (df[\"time\"] - epoch).apply(lambda x: int(x.total_seconds() * 1000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select three random id_combines files\n",
    "ids = [\"06_iPhone12-2023-03-16_13-46-58Manjavy_KirupaVelofahren\", \n",
    "       \"01_iPhone13pro-2023-03-21_16-55-47Etienne_RouletLaufen\", \n",
    "       \"01_iPhone13ProMax-2023-03-15_18-29-42Gabriel_TorresRennen\"]\n",
    "\n",
    "# create a dataframe with these three random files\n",
    "df_validation = df[df[\"id_combined\"].isin(ids)]\n",
    "\n",
    "# export the dataframe as a csv file\n",
    "df_validation.to_csv(\"validation-velo-laufen-rennen.csv\", index=False)\n",
    "\n",
    "# remove the validation data from the dataframe \n",
    "df = df[~df[\"id_combined\"].isin(ids)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unnötige Spalten und Class Encoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns that are not needed\n",
    "df = df.drop(columns=[\"id\", \"user\", \"id_combined\"])\n",
    "# get all types of the df\n",
    "le = LabelEncoder()\n",
    "# encode the classes\n",
    "df[\"class\"] = le.fit_transform(df[\"class\"])\n",
    "# print dictionary of the classes and its encoded values\n",
    "print(dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the window size and step size\n",
    "window_size = Parameters.window_size\n",
    "step_size = Parameters.step_size\n",
    "\n",
    "# Reshape X to 2D format (samples, features)\n",
    "X = df.values[:, 1:13]\n",
    "\n",
    "# Define y\n",
    "y = df[\"class\"].values\n",
    "\n",
    "# Create a sliding window of X with the specified window and step sizes\n",
    "X_windows = np.array(\n",
    "    [\n",
    "        X[i : i + window_size, :]\n",
    "        for i in range(0, X.shape[0] - window_size + 1, step_size)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Reshape X_windows to 3D format (samples, timesteps, features)\n",
    "timesteps = X_windows.shape[1]\n",
    "n_features = X_windows.shape[2]\n",
    "X_windows = X_windows.reshape(-1, timesteps, n_features)\n",
    "\n",
    "# Create the corresponding y labels for the sliding windows\n",
    "y_windows = np.array(\n",
    "    [y[i + window_size - 1] for i in range(0, X.shape[0] - window_size + 1, step_size)]\n",
    ")\n",
    "y_windows = to_categorical(y_windows, num_classes=6)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X_windows, y_windows, test_size=0.1, random_state=42, stratify=y_windows\n",
    ")\n",
    "\n",
    "# print Data shapes\n",
    "print(\"Data shapes:\")\n",
    "print(\"- X:         = {}\".format(X.shape))\n",
    "print(\"- y:         = {}\".format(y.shape))\n",
    "print(\"- X_train:   = {}\".format(x_train.shape))\n",
    "print(\"- y_train:   = {}\".format(y_train.shape))\n",
    "print(\"- X_test:    = {}\".format(x_test.shape))\n",
    "print(\"- y_test:    = {}\".format(y_test.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL-Modelle erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Deep Learning Models\n",
    "\n",
    "def create_model_1(name=\"model_1\"):\n",
    "    '''\n",
    "    CNN Model with 1 Convolutional Layer, 1 LSTM Layer and 1 Dense Layer \n",
    "    '''\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=64,\n",
    "                kernel_size=12,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.LSTM(100),\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_2(name=\"model_2\"):\n",
    "    '''\n",
    "    CNN Model with one Conv1D layer, one LSTM layer and one Dense layer, also using l2 regularization and dropout\n",
    "    '''\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=32,\n",
    "                kernel_size=8,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200)),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_3(name=\"model_3\"):\n",
    "    '''\n",
    "    CNN Model with 32 Filters combine with LSTM and one dense layer\n",
    "    '''\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=32,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.LSTM(100),\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_4(name=\"model_4\"):\n",
    "    '''\n",
    "    CNN Model with 64 Filters combined with LSTM and one dense layer\n",
    "    '''\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=64,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.LSTM(100),\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_5(name=\"model_5\"):\n",
    "    '''\n",
    "    CNN Model with 3 Conv1D Layers and 2 Dense Layers with Batch Normalization and L2 Regularization\n",
    "    '''\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # First Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=32,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Second Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=20, \n",
    "                kernel_size=2, \n",
    "                activation=\"relu\", \n",
    "                padding=\"same\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Third Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=10, \n",
    "                kernel_size=2, \n",
    "                activation=\"relu\", \n",
    "                padding=\"same\", \n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Flatten the results\n",
    "            tf.keras.layers.Flatten(),\n",
    "            # Add first dense layer\n",
    "            tf.keras.layers.Dense(units = 200, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(0.01),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add second dense layer\n",
    "            tf.keras.layers.Dense(units = 100, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(0.01),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add output Layer with Softmax Activation Funktion\n",
    "            tf.keras.layers.Dense(units = 6, \n",
    "                                  activation=\"softmax\"),\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    # Define optimizer, loss and metrics \n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_6(name=\"model_6\"):\n",
    "    '''\n",
    "    CNN Model with 2 Conv1D Layers and 3 Dense Layers with Batch Normalization and L2 Regularization\n",
    "    '''\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # First Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=64,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Second Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=32, \n",
    "                kernel_size=2, \n",
    "                activation=\"relu\", \n",
    "                padding=\"same\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Flatten the results\n",
    "            tf.keras.layers.Flatten(),\n",
    "            # Add first dense layer\n",
    "            tf.keras.layers.Dense(units = 200, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(0.01),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add second dense layer\n",
    "            tf.keras.layers.Dense(units = 100, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(0.01),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add third dense layer\n",
    "            tf.keras.layers.Dense(units = 50, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(0.01),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add output Layer with Softmax Activation Funktion\n",
    "            tf.keras.layers.Dense(units = 6, \n",
    "                                  activation=\"softmax\"),\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    # Define optimizer, loss and metrics \n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_7(name=\"model_7\", \n",
    "                   filterconv1 = 64, \n",
    "                   filterconv2 = 64, \n",
    "                   filterconv3 = 64, \n",
    "                   filterconv4 = 64,\n",
    "                   filterconv5 = 64, \n",
    "                   unitsdense1 = 200, \n",
    "                   unitsdense2 = 150, \n",
    "                   unitsdense3 = 100,\n",
    "                   unitsdense4 = 50,\n",
    "                   unitsdense5 = 25,\n",
    "                   l2_reg = 0.05):\n",
    "    '''\n",
    "    Check, if the deeper the model the better the results? \n",
    "    CNN Model with 5 Conv1D Layers and 5 Dense Layers with Batch Normalization and L2 Regularization\n",
    "    '''\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # First Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=filterconv1,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            ),\n",
    "            # Batchnormalization and Maxpooling\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Second Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=filterconv2, \n",
    "                kernel_size=2, \n",
    "                activation=\"relu\", \n",
    "                padding=\"same\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            ),\n",
    "            # Batchnormalization and Maxpooling\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Third Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=filterconv3, \n",
    "                kernel_size=2, \n",
    "                activation=\"relu\", \n",
    "                padding=\"same\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            ),\n",
    "            # Batchnormalization and Maxpooling\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Fourth Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=filterconv4,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            ),\n",
    "            # Batchnormalization and Maxpooling\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Fifth Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=filterconv5,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            ),\n",
    "            # Batchnormalization and Maxpooling\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Flatten the results\n",
    "            tf.keras.layers.Flatten(),\n",
    "            # Add first dense layer\n",
    "            tf.keras.layers.Dense(units = unitsdense1, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add second dense layer\n",
    "            tf.keras.layers.Dense(units = unitsdense2, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add third dense layer\n",
    "            tf.keras.layers.Dense(units = unitsdense3, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add fourth dense layer\n",
    "            tf.keras.layers.Dense(units = unitsdense4,\n",
    "                                  activation=\"relu\",\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # add fifth dense layer\n",
    "            tf.keras.layers.Dense(units = unitsdense5,\n",
    "                                  activation=\"relu\",\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "        \n",
    "            # Add output Layer with Softmax Activation Funktion\n",
    "            tf.keras.layers.Dense(units = 6, \n",
    "                                  activation=\"softmax\"),\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    # Define optimizer, loss and metrics \n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL-Modelle Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_history = None  # Keep track of the best model's history\n",
    "model_histories = []\n",
    "models = [create_model_6] # Add all models to this list\n",
    "best_model = None\n",
    "num_folds = Parameters.number_folds\n",
    "tscv = TimeSeriesSplit(n_splits=num_folds)\n",
    "fold_acc_scores = []\n",
    "\n",
    "for i, (train, test) in enumerate(tscv.split(x_train)):\n",
    "    logging.info(f\"Fold {i+1}\")\n",
    "    train_x, train_y = x_train[train], y_train[train]\n",
    "    test_x, test_y = x_train[test], y_train[test]\n",
    "\n",
    "    fold_histories = []\n",
    "\n",
    "    for j, model_creator in enumerate(models):\n",
    "        print(f\"Model {model_creator.__name__}\")\n",
    "        model_name = f\"Model_{j+1}_Fold_{i+1}\"\n",
    "        model = model_creator(name=model_name)\n",
    "        logging.info(f\"Model {j+1}\")\n",
    "        history = model.fit(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            epochs=Parameters.epochs,\n",
    "            batch_size=Parameters.batch_size,\n",
    "            validation_data=(test_x, test_y),\n",
    "            verbose=Parameters.verbosity,\n",
    "        )\n",
    "        test_loss, acc, prec, recal = model.evaluate(\n",
    "            test_x, test_y, verbose=Parameters.verbosity\n",
    "        )\n",
    "        logging.info(f\"Validation accuracy: {acc}\")\n",
    "\n",
    "        fold_histories.append(history.history)\n",
    "\n",
    "        for epoch in range(Parameters.epochs):\n",
    "            # Log accuracy after each epoch\n",
    "            acc_epoch = history.history[\"val_accuracy\"][epoch]\n",
    "            logging.info(f\"Epoch {epoch + 1}, Validation accuracy: {acc_epoch}\")\n",
    "        fold_acc_scores.append((i, j, acc))\n",
    "\n",
    "        if best_model_history is None or acc > best_model_acc:\n",
    "            best_model_history = history\n",
    "            best_model = model  # Store the trained model instance\n",
    "            best_model_acc = acc\n",
    "\n",
    "    model_histories.append(fold_histories)\n",
    "\n",
    "# Find the best model\n",
    "best_model_index = np.argmax([score[2] for score in fold_acc_scores])\n",
    "best_fold_idx, best_model_idx, _ = max(fold_acc_scores, key=lambda x: x[2])\n",
    "best_model_history = model_histories[best_fold_idx][best_model_idx]\n",
    "print(best_model.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verlauf von Accuracy und Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 subplots one for model accuracy and another for model loss\n",
    "fig, axs = plt.subplots(2, figsize=(15, 10))\n",
    "# Summarize history for accuracy\n",
    "axs[0].plot(best_model_history[\"accuracy\"])\n",
    "axs[0].plot(best_model_history[\"val_accuracy\"])\n",
    "axs[0].set_title(\"Model Accuracy\")\n",
    "axs[0].set_ylabel(\"Accuracy\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "# Summarize history for loss\n",
    "axs[1].plot(best_model_history[\"loss\"])\n",
    "axs[1].plot(best_model_history[\"val_loss\"])\n",
    "axs[1].set_title(\"Model Loss\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusions Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create for x_train and x_test confusion matrix\n",
    "y_pred = best_model.predict(x_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = accuracy_score(y_true = y_test_labels, y_pred = y_pred_labels)\n",
    "\n",
    "cm = confusion_matrix(y_true = y_test_labels, y_pred = y_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "\n",
    "# Create subplot for test confusion matrix\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7.5))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax1, colorbar=False)\n",
    "ax1.set_title(\"Test Confusion Matrix \\n \\\n",
    "                Accuracy: {:.2f}%, Datapoints: {}\".format(accuracy * 100, x_test.shape[0]), fontsize=18)\n",
    "ax1.set_xticklabels(le.classes_, rotation=45)\n",
    "\n",
    "# Create for x_train and x_test confusion matrix\n",
    "y_pred = best_model.predict(x_train)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_true = y_train_labels, y_pred = y_pred_labels)\n",
    "cm = confusion_matrix(y_true = y_train_labels, y_pred = y_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "\n",
    "# Create subplot for train confusion matrix\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax2, colorbar=False)\n",
    "# add to title number of observations\n",
    "ax2.set_title(\"Train Confusion Matrix \\n \\\n",
    "              Accuracy: {:.2f}%, Datapoints: {}\".format(accuracy * 100, x_train.shape[0]), fontsize=18)\n",
    "\n",
    "ax2.set_xticklabels(le.classes_, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export DL-Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export Model to json Tensorflow file\n",
    "import json\n",
    "\n",
    "best_model.save(\"saved_model/sensor_model.h5\")\n",
    "\n",
    "model = tf.keras.models.load_model(\"saved_model/sensor_model.h5\")\n",
    "\n",
    "# Save model architecture to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save model weights to a JSON file\n",
    "weights = model.get_weights()\n",
    "weights_as_list = [w.tolist() for w in weights]\n",
    "with open(\"weights.json\", \"w\") as f:\n",
    "    json.dump(weights_as_list, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell Einlesen und predicten auf Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class validate_unseen_data():\n",
    "    def __init__(self, model_path=\"saved_model/sensor_model.h5\"):\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        self.csv_path = \"validation-velo-laufen-rennen.csv\"\n",
    "        \n",
    "    def predict_classes(self, file=\"06_iPhone12-2023-03-16_13-46-58Manjavy_KirupaVelofahren\", expected=''):\n",
    "        df_val = pd.read_csv(self.csv_path)\n",
    "        # Filter the rows where its velo in id_combined\n",
    "        df_val = df_val[df_val[\"id_combined\"].str.contains(file)]\n",
    "        df_val = df_val.drop(columns=[\"id\", \"user\", \"id_combined\"])\n",
    "        # convert the string time column to datetime\n",
    "        epoch = pd.Timestamp(\"1970-01-01\")\n",
    "        df_val[\"time\"] = pd.to_datetime(df_val[\"time\"])\n",
    "        df_val[\"time\"] = (df_val[\"time\"] - epoch).apply(\n",
    "            lambda x: int(x.total_seconds() * 1000)\n",
    "        )\n",
    "        # get all types of the df\n",
    "        le = LabelEncoder()\n",
    "        df_val[\"class\"] = le.fit_transform(df_val[\"class\"])\n",
    "\n",
    "        # Set the window size and step size\n",
    "        window_size = 300\n",
    "        step_size = 100\n",
    "\n",
    "        # Reshape X to 2D format (samples, features)\n",
    "        X = df_val.values[:, 1:13]\n",
    "        y = df[\"class\"].values\n",
    "\n",
    "        # Create a sliding window of X with the specified window and step sizes\n",
    "        X_windows = np.array(\n",
    "            [\n",
    "                X[i : i + window_size, :]\n",
    "                for i in range(0, X.shape[0] - window_size + 1, step_size)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Reshape X_windows to 3D format (samples, timesteps, features)\n",
    "        timesteps = X_windows.shape[1]\n",
    "        n_features = X_windows.shape[2]\n",
    "        X_windows = X_windows.reshape(-1, timesteps, n_features)\n",
    "\n",
    "        # Create the corresponding y labels for the sliding windows\n",
    "        y_windows = np.array(\n",
    "            [y[i + window_size - 1] for i in range(0, X.shape[0] - window_size + 1, step_size)]\n",
    "        )\n",
    "        y_windows = to_categorical(y_windows, num_classes=6)\n",
    "\n",
    "        # predict validation data\n",
    "        y_pred_probs = model.predict(X_windows)\n",
    "\n",
    "        # Get the predicted class labels for each input window\n",
    "        y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "        # Print the predicted class labels\n",
    "        # get the median of the predicted labels\n",
    "        sol = np.median(y_pred_labels)\n",
    "\n",
    "        class_counts = np.bincount(y_pred_labels)\n",
    "        for i, count in enumerate(class_counts):\n",
    "            print(f\"Class {i} count: {count}\")\n",
    "        \n",
    "        return (sol, expected)\n",
    "\n",
    "median = validate_unseen_data().predict_classes('06_iPhone12-2023-03-16_13-46-58Manjavy_KirupaVelofahren', expected='velo')\n",
    "median2 = validate_unseen_data().predict_classes('01_iPhone13ProMax-2023-03-15_18-29-42Gabriel_TorresRennen', expected='rennen')\n",
    "median3 = validate_unseen_data().predict_classes('01_iPhone13pro-2023-03-21_16-55-47Etienne_RouletLaufen', expected='laufen')\n",
    "\n",
    "print((median, median2, median3))\n",
    "\n",
    "\n",
    "\n",
    "print(dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Conclusion is that the model is not overfitted but Gabriel is more a Stepper than a Runner ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
