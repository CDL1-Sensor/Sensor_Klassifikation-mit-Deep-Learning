{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensor Based Activity Recoginition \n",
    "Challenge: cdl1 - Sensor based Activity Recognition  \n",
    "Team: Lea BÃ¼tler, Manjavy Kirupa, Etienne Roulet, Si Ben Tran  \n",
    "\n",
    "Aufgabe: DL Modell erstellen\n",
    "\n",
    "Hier in diesem Notebook erstellen wir unsere Deep Learning Modelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices())\n",
    "tf.debugging.set_log_device_placement(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp310-cp310-win_amd64.whl (8.3 MB)\n",
      "     ---------------------------------------- 8.3/8.3 MB 7.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\super\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\super\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.2 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\super\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\super\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\super\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\super\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\super\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-directml-plugin\n",
      "  Downloading tensorflow_directml_plugin-0.4.0.dev230202-cp310-cp310-win_amd64.whl (9.0 MB)\n",
      "     ---------------------------------------- 9.0/9.0 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.53.0-cp310-cp310-win_amd64.whl (3.9 MB)\n",
      "     ---------------------------------------- 3.9/3.9 MB 4.2 MB/s eta 0:00:00\n",
      "Collecting tensorflow-cpu==2.10.0\n",
      "  Downloading tensorflow_cpu-2.10.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Collecting wheel>=0.26\n",
      "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "Collecting tensorflow-intel==2.10.0\n",
      "  Downloading tensorflow_intel-2.10.0-cp310-cp310-win_amd64.whl (262.7 MB)\n",
      "     -------------------------------------- 262.7/262.7 MB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\super\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0->tensorflow-directml-plugin) (23.0)\n",
      "Collecting tensorboard<2.11,>=2.10\n",
      "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
      "     ---------------------------------------- 5.9/5.9 MB 6.7 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting keras<2.11,>=2.10.0\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 8.2 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-win_amd64.whl (895 kB)\n",
      "     -------------------------------------- 895.7/895.7 kB 7.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\super\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0->tensorflow-directml-plugin) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\super\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0->tensorflow-directml-plugin) (4.5.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 7.3 MB/s eta 0:00:00\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.6/42.6 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.11,>=2.10.0\n",
      "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
      "     -------------------------------------- 438.7/438.7 kB 9.1 MB/s eta 0:00:00\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\super\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0->tensorflow-directml-plugin) (65.5.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\super\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0->tensorflow-directml-plugin) (1.23.5)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "     ---------------------------------------- 2.6/2.6 MB 8.0 MB/s eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\super\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0->tensorflow-directml-plugin) (2.28.2)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.17.2-py2.py3-none-any.whl (178 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\super\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0->tensorflow-directml-plugin) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\super\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0->tensorflow-directml-plugin) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\super\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0->tensorflow-directml-plugin) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\super\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0->tensorflow-directml-plugin) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\super\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow-intel==2.10.0->tensorflow-cpu==2.10.0->tensorflow-directml-plugin) (2.1.2)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: tensorboard-plugin-wit, pyasn1, libclang, keras, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, pyasn1-modules, protobuf, opt-einsum, oauthlib, markdown, keras-preprocessing, h5py, grpcio, google-pasta, gast, cachetools, absl-py, requests-oauthlib, google-auth, astunparse, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow-cpu, tensorflow-directml-plugin\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.17.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.53.0 h5py-3.8.0 keras-2.10.0 keras-preprocessing-1.1.2 libclang-16.0.0 markdown-3.4.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-cpu-2.10.0 tensorflow-directml-plugin-0.4.0.dev230202 tensorflow-estimator-2.10.0 tensorflow-intel-2.10.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.2.0 werkzeug-2.2.3 wheel-0.40.0 wrapt-1.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow-directml-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "# datetime as filename for logging\n",
    "now = datetime.now()\n",
    "date_time_string = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, filename = f\"{date_time_string}.txt\", filemode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Parameters():\n",
    "    batch_size: int = 1024\n",
    "    epochs: int = 10\n",
    "    verbosity: int = 1\n",
    "    step_size: int = 374\n",
    "    number_folds: int = 2\n",
    "    output_size: int = 6\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>Accelerometer_x</th>\n",
       "      <th>Accelerometer_y</th>\n",
       "      <th>Accelerometer_z</th>\n",
       "      <th>Gyroscope_x</th>\n",
       "      <th>Gyroscope_y</th>\n",
       "      <th>Gyroscope_z</th>\n",
       "      <th>Magnetometer_x</th>\n",
       "      <th>Magnetometer_y</th>\n",
       "      <th>Magnetometer_z</th>\n",
       "      <th>Orientation_qx</th>\n",
       "      <th>Orientation_qy</th>\n",
       "      <th>Orientation_qz</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>class</th>\n",
       "      <th>id_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-27 15:02:17.156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.722</td>\n",
       "      <td>1.278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.21775</td>\n",
       "      <td>-0.719579</td>\n",
       "      <td>0.631111</td>\n",
       "      <td>01_SamsungA22-2023-02-27_15-02-03</td>\n",
       "      <td>Ben_Tran</td>\n",
       "      <td>Laufen</td>\n",
       "      <td>01_SamsungA22-2023-02-27_15-02-03Ben_TranLaufen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     time  Accelerometer_x  Accelerometer_y  \\\n",
       "0           1  2023-02-27 15:02:17.156              0.0            9.722   \n",
       "\n",
       "   Accelerometer_z  Gyroscope_x  Gyroscope_y  Gyroscope_z  Magnetometer_x  \\\n",
       "0            1.278          0.0          0.0          0.0             0.0   \n",
       "\n",
       "   Magnetometer_y  Magnetometer_z  Orientation_qx  Orientation_qy  \\\n",
       "0             0.0             0.0        -0.21775       -0.719579   \n",
       "\n",
       "   Orientation_qz                                 id      user   class  \\\n",
       "0        0.631111  01_SamsungA22-2023-02-27_15-02-03  Ben_Tran  Laufen   \n",
       "\n",
       "                                       id_combined  \n",
       "0  01_SamsungA22-2023-02-27_15-02-03Ben_TranLaufen  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading Data\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection as ms\n",
    "\n",
    "# read the CSV file into a DataFrame\n",
    "df = pd.read_csv(\"Alle_Messungen_trimmed.csv\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"id\", \"user\", \"id_combined\"])\n",
    "df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "# get all types of the df\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "df['time'] = df['time'].astype('int64')//1e9\n",
    "le = LabelEncoder()\n",
    "df[\"class\"] = le.fit_transform(df[\"class\"])\n",
    "X_train, X_test, y_train, y_test = ms.train_test_split(df.values[:, 1:13], df.values[:, 13], test_size=0.2, random_state=42)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((df.values[:, 1:13], df.values[:, 13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 9s 8ms/step - loss: 3.0560 - accuracy: 0.2518 - val_loss: 1.5914 - val_accuracy: 0.4260\n",
      "Epoch 2/10\n",
      " 139/1000 [===>..........................] - ETA: 4s - loss: 1.6541 - accuracy: 0.3198"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mfor\u001b[39;00m j, model \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(models):\n\u001b[0;32m     64\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mModel \u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_x, train_y, epochs\u001b[39m=\u001b[39;49mParameters\u001b[39m.\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mParameters\u001b[39m.\u001b[39;49mbatch_size, validation_data\u001b[39m=\u001b[39;49m(test_x, test_y), verbose\u001b[39m=\u001b[39;49mParameters\u001b[39m.\u001b[39;49mverbosity)\n\u001b[0;32m     66\u001b[0m     _, acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(test_x, test_y, verbose\u001b[39m=\u001b[39mParameters\u001b[39m.\u001b[39mverbosity)\n\u001b[0;32m     67\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mValidation accuracy: \u001b[39m\u001b[39m{\u001b[39;00macc\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[0;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[0;32m    342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     hook(batch, logs)\n\u001b[0;32m    390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   1158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    629\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mc:\\Users\\super\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[0;32m   1124\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Template\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import dataclasses\n",
    "\n",
    "# Load data and preprocess\n",
    "# split train dataset into x_train and y_train\n",
    "x_train = X_train\n",
    "x_test = X_test\n",
    "y_train = y_train\n",
    "y_test = y_test\n",
    "    \n",
    "\n",
    "# Something like this as first Model\n",
    "def create_model_1():\n",
    "    model = keras.Sequential([\n",
    "            tf.keras.layers.Dense(16, activation='relu', input_shape=(12,)),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(8, activation='relu', input_shape=(12,)),\n",
    "            tf.keras.layers.Dense(6, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Something like this as second Model\n",
    "def create_model_2():\n",
    "    model = keras.Sequential([\n",
    "        tf.keras.layers.Reshape((-1, 12), input_shape=(12,)),\n",
    "        tf.keras.layers.Conv1D(32, 5, strides=2, padding='same', activation='relu'),\n",
    "        tf.keras.layers.Conv1D(16, 3, strides=2, padding='same', activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(6, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Something like this as Third Model\n",
    "def create_model_3():\n",
    "    model = keras.Sequential([\n",
    "        tf.keras.layers.Reshape((-1, 12), input_shape=(12,)),\n",
    "        tf.keras.layers.Conv1D(32, 5, strides=2, padding='same', activation='relu'),\n",
    "        tf.keras.layers.Conv1D(16, 3, strides=2, padding='same', activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(6, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Perform cross-validation\n",
    "models = [create_model_1(), create_model_2(), create_model_3()]\n",
    "num_folds = Parameters.number_folds\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "fold_acc_scores = []\n",
    "for i, (train, test) in enumerate(kfold.split(x_train, y_train)):\n",
    "    logging.info(f'Fold {i+1}')\n",
    "    train_x, train_y = x_train[train], y_train[train]\n",
    "    test_x, test_y = x_train[test], y_train[test]\n",
    "    for j, model in enumerate(models):\n",
    "        logging.info(f'Model {j+1}')\n",
    "        history = model.fit(train_x, train_y, epochs=Parameters.epochs, batch_size=Parameters.batch_size, validation_data=(test_x, test_y), verbose=Parameters.verbosity)\n",
    "        _, acc = model.evaluate(test_x, test_y, verbose=Parameters.verbosity)\n",
    "        logging.info(f'Validation accuracy: {acc}')\n",
    "        for epoch in range(Parameters.epochs):\n",
    "            # Log accuracy after each epoch\n",
    "            acc_epoch = history.history['val_accuracy'][epoch]\n",
    "            logging.info(f'Epoch {epoch + 1}, Validation accuracy: {acc_epoch}')\n",
    "        fold_acc_scores.append((i, j, acc))\n",
    "\n",
    "# Find the best model\n",
    "best_model_index = np.argmax([score[2] for score in fold_acc_scores])\n",
    "logging.info(fold_acc_scores)\n",
    "logging.info(best_model_index)\n",
    "best_model = models[fold_acc_scores[best_model_index][1]]\n",
    "logging.info(f'fold acc score: {fold_acc_scores}')\n",
    "logging.info(f'Best model is model {best_model_index+1}')\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_loss, test_acc = best_model.evaluate(x_test, y_test)\n",
    "logging.info('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Baseline vs best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graph for learning curve and loss curve from baseline and best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarise best model\n",
    "best_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred.argmax(axis=1))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Model for using in tensorflow.js\n",
    "!mkdir -p saved_model\n",
    "best_model.save('saved_model/sensor_model')\n",
    "best_model.save('saved_model/sensor_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weight for the Js Model\n",
    "best_model.save_weights('./checkpoints/my_checkpoint')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload model to server to download it on tensorflow js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('notebook_env.db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
