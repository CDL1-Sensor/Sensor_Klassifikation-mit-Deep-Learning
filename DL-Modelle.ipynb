{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensor Based Activity Recoginition \n",
    "Challenge: cdl1 - Sensor based Activity Recognition  \n",
    "Team: Lea BÃ¼tler, Manjavy Kirupa, Etienne Roulet, Si Ben Tran  \n",
    "\n",
    "Aufgabe: DL Modell erstellen\n",
    "\n",
    "Hier in diesem Notebook erstellen wir unsere Deep Learning Modelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "# datetime as filename for logging\n",
    "now = datetime.now()\n",
    "date_time_string = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, filename=f\"{date_time_string}.txt\", filemode=\"a\"\n",
    ")\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  c:\\Users\\super\\miniconda3\\envs\\tf_gpu\\python.exe -m pip <command> [options]\n",
      "\n",
      "Commands:\n",
      "  install                     Install packages.\n",
      "  download                    Download packages.\n",
      "  uninstall                   Uninstall packages.\n",
      "  freeze                      Output installed packages in requirements format.\n",
      "  inspect                     Inspect the python environment.\n",
      "  list                        List installed packages.\n",
      "  show                        Show information about installed packages.\n",
      "  check                       Verify installed packages have compatible dependencies.\n",
      "  config                      Manage local and global configuration.\n",
      "  search                      Search PyPI for packages.\n",
      "  cache                       Inspect and manage pip's wheel cache.\n",
      "  index                       Inspect information available from package indexes.\n",
      "  wheel                       Build wheels from your requirements.\n",
      "  hash                        Compute hashes of package archives.\n",
      "  completion                  A helper command used for command completion.\n",
      "  debug                       Show information useful for debugging.\n",
      "  help                        Show help for commands.\n",
      "\n",
      "General Options:\n",
      "  -h, --help                  Show help.\n",
      "  --debug                     Let unhandled exceptions propagate outside the\n",
      "                              main subroutine, instead of logging them to\n",
      "                              stderr.\n",
      "  --isolated                  Run pip in an isolated mode, ignoring\n",
      "                              environment variables and user configuration.\n",
      "  --require-virtualenv        Allow pip to only run in a virtual environment;\n",
      "                              exit with an error otherwise.\n",
      "  --python <python>           Run pip with the specified Python interpreter.\n",
      "  -v, --verbose               Give more output. Option is additive, and can be\n",
      "                              used up to 3 times.\n",
      "  -V, --version               Show version and exit.\n",
      "  -q, --quiet                 Give less output. Option is additive, and can be\n",
      "                              used up to 3 times (corresponding to WARNING,\n",
      "                              ERROR, and CRITICAL logging levels).\n",
      "  --log <path>                Path to a verbose appending log.\n",
      "  --no-input                  Disable prompting for input.\n",
      "  --proxy <proxy>             Specify a proxy in the form\n",
      "                              scheme://[user:passwd@]proxy.server:port.\n",
      "  --retries <retries>         Maximum number of retries each connection should\n",
      "                              attempt (default 5 times).\n",
      "  --timeout <sec>             Set the socket timeout (default 15 seconds).\n",
      "  --exists-action <action>    Default action when a path already exists:\n",
      "                              (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort.\n",
      "  --trusted-host <hostname>   Mark this host or host:port pair as trusted,\n",
      "                              even though it does not have valid or any HTTPS.\n",
      "  --cert <path>               Path to PEM-encoded CA certificate bundle. If\n",
      "                              provided, overrides the default. See 'SSL\n",
      "                              Certificate Verification' in pip documentation\n",
      "                              for more information.\n",
      "  --client-cert <path>        Path to SSL client certificate, a single file\n",
      "                              containing the private key and the certificate\n",
      "                              in PEM format.\n",
      "  --cache-dir <dir>           Store the cache data in <dir>.\n",
      "  --no-cache-dir              Disable the cache.\n",
      "  --disable-pip-version-check\n",
      "                              Don't periodically check PyPI to determine\n",
      "                              whether a new version of pip is available for\n",
      "                              download. Implied with --no-index.\n",
      "  --no-color                  Suppress colored output.\n",
      "  --no-python-version-warning\n",
      "                              Silence deprecation warnings for upcoming\n",
      "                              unsupported Pythons.\n",
      "  --use-feature <feature>     Enable new functionality, that may be backward\n",
      "                              incompatible.\n",
      "  --use-deprecated <feature>  Enable deprecated functionality, that will be\n",
      "                              removed in the future.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# Static Parameters\n",
    "@dataclass\n",
    "class Parameters:\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 10\n",
    "    verbosity: str = \"auto\"\n",
    "    step_size: int = 374\n",
    "    number_folds: int = 6\n",
    "    output_size: int = 6\n",
    "    window_size = 300\n",
    "    step_size = 100\n",
    "    data_augmentation  = True\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>Accelerometer_x</th>\n",
       "      <th>Accelerometer_y</th>\n",
       "      <th>Accelerometer_z</th>\n",
       "      <th>Gyroscope_x</th>\n",
       "      <th>Gyroscope_y</th>\n",
       "      <th>Gyroscope_z</th>\n",
       "      <th>Magnetometer_x</th>\n",
       "      <th>Magnetometer_y</th>\n",
       "      <th>Magnetometer_z</th>\n",
       "      <th>Orientation_qx</th>\n",
       "      <th>Orientation_qy</th>\n",
       "      <th>Orientation_qz</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>class</th>\n",
       "      <th>id_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-02-27 15:02:17.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.722</td>\n",
       "      <td>1.278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.21775</td>\n",
       "      <td>-0.719579</td>\n",
       "      <td>0.631111</td>\n",
       "      <td>01_SamsungA22-2023-02-27_15-02-03</td>\n",
       "      <td>Ben_Tran</td>\n",
       "      <td>Laufen</td>\n",
       "      <td>01_SamsungA22-2023-02-27_15-02-03Ben_TranLaufen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-02-27 15:02:31.574</td>\n",
       "      <td>-0.728</td>\n",
       "      <td>9.779</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.42065</td>\n",
       "      <td>0.568356</td>\n",
       "      <td>-0.598058</td>\n",
       "      <td>01_SamsungA22-2023-02-27_15-02-03</td>\n",
       "      <td>Ben_Tran</td>\n",
       "      <td>Laufen</td>\n",
       "      <td>01_SamsungA22-2023-02-27_15-02-03Ben_TranLaufen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-02-27 15:02:16.685</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.5</td>\n",
       "      <td>-13.143750</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>01_SamsungA22-2023-02-27_15-02-03</td>\n",
       "      <td>Ben_Tran</td>\n",
       "      <td>Laufen</td>\n",
       "      <td>01_SamsungA22-2023-02-27_15-02-03Ben_TranLaufen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-02-27 15:02:32.295</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.012501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.587502</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>01_SamsungA22-2023-02-27_15-02-03</td>\n",
       "      <td>Ben_Tran</td>\n",
       "      <td>Laufen</td>\n",
       "      <td>01_SamsungA22-2023-02-27_15-02-03Ben_TranLaufen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2023-02-27 15:02:33.285</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-22.612501</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.268751</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>01_SamsungA22-2023-02-27_15-02-03</td>\n",
       "      <td>Ben_Tran</td>\n",
       "      <td>Laufen</td>\n",
       "      <td>01_SamsungA22-2023-02-27_15-02-03Ben_TranLaufen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     time  Accelerometer_x  Accelerometer_y   \n",
       "0           1  2023-02-27 15:02:17.156            0.000            9.722  \\\n",
       "1           2  2023-02-27 15:02:31.574           -0.728            9.779   \n",
       "2           3  2023-02-27 15:02:16.685            0.000            0.000   \n",
       "3           4  2023-02-27 15:02:32.295            0.000            0.000   \n",
       "4           5  2023-02-27 15:02:33.285            0.000            0.000   \n",
       "\n",
       "   Accelerometer_z  Gyroscope_x  Gyroscope_y  Gyroscope_z  Magnetometer_x   \n",
       "0            1.278          0.0          0.0          0.0        0.000000  \\\n",
       "1            0.000          0.0          0.0          0.0        0.000000   \n",
       "2            0.000          0.0          0.0          0.0        0.000000   \n",
       "3            0.000          0.0          0.0          0.0      -13.012501   \n",
       "4            0.000          0.0          0.0          0.0      -22.612501   \n",
       "\n",
       "   Magnetometer_y  Magnetometer_z  Orientation_qx  Orientation_qy   \n",
       "0             0.0        0.000000        -0.21775       -0.719579  \\\n",
       "1             0.0        0.000000        -0.42065        0.568356   \n",
       "2            43.5      -13.143750         0.00000        0.000000   \n",
       "3             0.0       20.587502         0.00000        0.000000   \n",
       "4             0.0       20.268751         0.00000        0.000000   \n",
       "\n",
       "   Orientation_qz                                 id      user   class   \n",
       "0        0.631111  01_SamsungA22-2023-02-27_15-02-03  Ben_Tran  Laufen  \\\n",
       "1       -0.598058  01_SamsungA22-2023-02-27_15-02-03  Ben_Tran  Laufen   \n",
       "2        0.000000  01_SamsungA22-2023-02-27_15-02-03  Ben_Tran  Laufen   \n",
       "3        0.000000  01_SamsungA22-2023-02-27_15-02-03  Ben_Tran  Laufen   \n",
       "4        0.000000  01_SamsungA22-2023-02-27_15-02-03  Ben_Tran  Laufen   \n",
       "\n",
       "                                       id_combined  \n",
       "0  01_SamsungA22-2023-02-27_15-02-03Ben_TranLaufen  \n",
       "1  01_SamsungA22-2023-02-27_15-02-03Ben_TranLaufen  \n",
       "2  01_SamsungA22-2023-02-27_15-02-03Ben_TranLaufen  \n",
       "3  01_SamsungA22-2023-02-27_15-02-03Ben_TranLaufen  \n",
       "4  01_SamsungA22-2023-02-27_15-02-03Ben_TranLaufen  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Sensor_Data-Wrangling-und-EDA/Alle_Messungen_trimmed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time umwandeln "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2559982, 18)\n"
     ]
    }
   ],
   "source": [
    "# convert the string time column to datetime\n",
    "epoch = pd.Timestamp(\"1970-01-01\")\n",
    "\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "df[\"time\"] = (df[\"time\"] - epoch).apply(lambda x: int(x.total_seconds() * 1000))\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select three random id_combines files\n",
    "ids = [\"06_iPhone12-2023-03-16_13-46-58Manjavy_KirupaVelofahren\", \"01_iPhone13pro-2023-03-21_16-55-47Etienne_RouletLaufen\", \"01_iPhone13ProMax-2023-03-15_18-29-42Gabriel_TorresRennen\"]\n",
    "df_validation = df[df[\"id_combined\"].isin(ids)]\n",
    "\n",
    "# export to csv \n",
    "df_validation.to_csv(\"validation-velo-laufen-rennen.csv\", index=False)\n",
    "\n",
    "# remove the validation data from the dataframe \n",
    "df = df[~df[\"id_combined\"].isin(ids)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unnÃ¶tige Spalten und Class Encoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Laufen': 0, 'Rennen': 1, 'Sitzen': 2, 'Stehen': 3, 'Treppenlaufen': 4, 'Velofahren': 5}\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(columns=[\"id\", \"user\", \"id_combined\"])\n",
    "df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "# get all types of the df\n",
    "le = LabelEncoder()\n",
    "df[\"class\"] = le.fit_transform(df[\"class\"])\n",
    "# print dictionary of the classes and its encoded values\n",
    "print(dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def add_noise(time_series, noise_factor):\n",
    "    noise = np.random.randn(len(time_series)) * noise_factor\n",
    "    return time_series + noise\n",
    "\n",
    "# Flip time series\n",
    "def flip_time_series(time_series):\n",
    "    return np.flip(time_series)\n",
    "\n",
    "# Scale magnitude\n",
    "def scale_magnitude(time_series, scaling_factor):\n",
    "    return time_series * scaling_factor\n",
    "\n",
    "# Augment the data\n",
    "def augment_data(X, y, noise_factor, scaling_factor, flip_probability):\n",
    "    augmented_X = []\n",
    "    augmented_y = []\n",
    "    for i in range(X.shape[0]):\n",
    "        x_sample = X[i]\n",
    "        y_sample = y[i]\n",
    "\n",
    "        # Add noise\n",
    "        x_sample_noisy = add_noise(x_sample, noise_factor)\n",
    "        augmented_X.append(x_sample_noisy)\n",
    "        augmented_y.append(y_sample)\n",
    "\n",
    "        # Scale magnitude\n",
    "        x_sample_scaled = scale_magnitude(x_sample, scaling_factor)\n",
    "        augmented_X.append(x_sample_scaled)\n",
    "        augmented_y.append(y_sample)\n",
    "\n",
    "        # Flip time series\n",
    "        if random.random() < flip_probability:\n",
    "            x_sample_flipped = flip_time_series(x_sample)\n",
    "            augmented_X.append(x_sample_flipped)\n",
    "            augmented_y.append(y_sample)\n",
    "\n",
    "    return np.array(augmented_X), np.array(augmented_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the window size and step size\n",
    "\n",
    "window_size = Parameters.window_size\n",
    "step_size = Parameters.step_size\n",
    "# Reshape X to 2D format (samples, features)\n",
    "X = df.values[:, 1:13]\n",
    "# Define y\n",
    "y = df[\"class\"].values\n",
    "X_windows = np.array(\n",
    "        [\n",
    "            X[i : i + window_size, :]\n",
    "            for i in range(0, X.shape[0] - window_size + 1, step_size)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Reshape X_windows to 3D format (samples, timesteps, features)\n",
    "timesteps = X_windows.shape[1]\n",
    "n_features = X_windows.shape[2]\n",
    "X_windows = X_windows.reshape(-1, timesteps, n_features)\n",
    "\n",
    "if (Parameters.data_augmentation == False):\n",
    "\n",
    "    # Create a sliding window of X with the specified window and step sizes\n",
    "    \n",
    "\n",
    "    # Create the corresponding y labels for the sliding windows\n",
    "    y_windows = np.array(\n",
    "        [y[i + window_size - 1] for i in range(0, X.shape[0] - window_size + 1, step_size)]\n",
    "    )\n",
    "    y_windows = to_categorical(y_windows, num_classes=6)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_windows, y_windows, test_size=0.1, random_state=42, stratify=y_windows\n",
    "    )\n",
    "elif (Parameters.data_augmentation == True):\n",
    "        # Augmentation parameters\n",
    "    noise_factor = 0.1\n",
    "    scaling_factor = 1.5\n",
    "    flip_probability = 0.5\n",
    "    \n",
    "    # Augment the data\n",
    "    X_augmented, y_augmented = augment_data(X, y, noise_factor, scaling_factor, flip_probability)\n",
    "    \n",
    "    # Create sliding windows for the augmented data\n",
    "    X_windows_augmented = np.array(\n",
    "        [\n",
    "            X_augmented[i : i + window_size, :]\n",
    "            for i in range(0, X_augmented.shape[0] - window_size + 1, step_size)\n",
    "        ]\n",
    "    )\n",
    "    X_windows_augmented = X_windows_augmented.reshape(-1, timesteps, n_features)\n",
    "    \n",
    "    y_windows_augmented = np.array(\n",
    "        [y_augmented[i + window_size - 1] for i in range(0, X_augmented.shape[0] - window_size + 1, step_size)]\n",
    "    )\n",
    "    y_windows_augmented = to_categorical(y_windows_augmented, num_classes=6)\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_validation, y_train, y_validaton = train_test_split(\n",
    "        X_windows_augmented, y_windows_augmented, test_size=0.2, random_state=42, stratify=y_windows_augmented\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2522292, 12),\n",
       " (2522292,),\n",
       " (50445, 300, 12),\n",
       " (50445, 6),\n",
       " (12612, 300, 12),\n",
       " (12612, 6))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape, X_train.shape, y_train.shape, X_validation.shape, y_validaton.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umschreibung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train\n",
    "x_validation = X_validation\n",
    "y_train = y_train\n",
    "y_validation = y_validaton"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL-Modelle erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Something like this as first Model\n",
    "\n",
    "def create_model_1(name=\"model_1\"):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=64,\n",
    "                kernel_size=12,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.LSTM(100),\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_2(name=\"model_2\"):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=32,\n",
    "                kernel_size=8,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300)),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_model_3(name=\"model_3\"):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=32,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.LSTM(100),\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_4(name=\"model_4\"):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=64,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.LSTM(100),\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_5(name=\"model_5\"):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=32,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=20, kernel_size=2, activation=\"relu\", padding=\"same\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=10, kernel_size=2, activation=\"relu\", padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # flatten output\n",
    "            tf.keras.layers.Flatten(),\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(180, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(100, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Change activation function based on the nature of the output\n",
    "            tf.keras.layers.Dense(6, activation=\"softmax\"),\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL-Modelle Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model create_model_2\n",
      "Epoch 1/10\n",
      "57/57 [==============================] - 10s 85ms/step - loss: 1.2647 - accuracy: 0.5073 - precision: 0.6776 - recall: 0.3236 - val_loss: 0.8955 - val_accuracy: 0.6737 - val_precision: 0.8204 - val_recall: 0.4797\n",
      "Epoch 2/10\n",
      "57/57 [==============================] - 4s 75ms/step - loss: 0.7582 - accuracy: 0.7249 - precision: 0.8017 - recall: 0.6269 - val_loss: 0.5700 - val_accuracy: 0.8032 - val_precision: 0.8541 - val_recall: 0.7549\n",
      "Epoch 3/10\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.5614 - accuracy: 0.7983 - precision: 0.8396 - recall: 0.7549 - val_loss: 0.4793 - val_accuracy: 0.8442 - val_precision: 0.8699 - val_recall: 0.8117\n",
      "Epoch 4/10\n",
      "57/57 [==============================] - 4s 76ms/step - loss: 0.4955 - accuracy: 0.8252 - precision: 0.8574 - recall: 0.7937 - val_loss: 0.4494 - val_accuracy: 0.8622 - val_precision: 0.8910 - val_recall: 0.8260\n",
      "Epoch 5/10\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.4245 - accuracy: 0.8568 - precision: 0.8799 - recall: 0.8305 - val_loss: 0.3715 - val_accuracy: 0.8838 - val_precision: 0.8982 - val_recall: 0.8708\n",
      "Epoch 6/10\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.3749 - accuracy: 0.8777 - precision: 0.8932 - recall: 0.8581 - val_loss: 0.3704 - val_accuracy: 0.8829 - val_precision: 0.9006 - val_recall: 0.8687\n",
      "Epoch 7/10\n",
      "57/57 [==============================] - 4s 76ms/step - loss: 0.3712 - accuracy: 0.8767 - precision: 0.8924 - recall: 0.8575 - val_loss: 0.3815 - val_accuracy: 0.8782 - val_precision: 0.8937 - val_recall: 0.8661\n",
      "Epoch 8/10\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.3647 - accuracy: 0.8804 - precision: 0.8965 - recall: 0.8659 - val_loss: 0.3525 - val_accuracy: 0.8808 - val_precision: 0.8953 - val_recall: 0.8725\n",
      "Epoch 9/10\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.2933 - accuracy: 0.9058 - precision: 0.9157 - recall: 0.8961 - val_loss: 0.3317 - val_accuracy: 0.8937 - val_precision: 0.9070 - val_recall: 0.8833\n",
      "Epoch 10/10\n",
      "57/57 [==============================] - 4s 74ms/step - loss: 0.4186 - accuracy: 0.8593 - precision: 0.8797 - recall: 0.8348 - val_loss: 0.3480 - val_accuracy: 0.8915 - val_precision: 0.9079 - val_recall: 0.8757\n",
      "226/226 [==============================] - 5s 22ms/step - loss: 0.3480 - accuracy: 0.8915 - precision: 0.9079 - recall: 0.8758\n",
      "Model create_model_2\n",
      "Epoch 1/10\n",
      "113/113 [==============================] - 10s 70ms/step - loss: 1.0256 - accuracy: 0.6155 - precision_1: 0.7595 - recall_1: 0.4728 - val_loss: 0.5563 - val_accuracy: 0.8139 - val_precision_1: 0.8639 - val_recall_1: 0.7499\n",
      "Epoch 2/10\n",
      "113/113 [==============================] - 7s 61ms/step - loss: 0.5241 - accuracy: 0.8181 - precision_1: 0.8523 - recall_1: 0.7838 - val_loss: 0.4826 - val_accuracy: 0.8361 - val_precision_1: 0.8836 - val_recall_1: 0.7824\n",
      "Epoch 3/10\n",
      "113/113 [==============================] - 7s 61ms/step - loss: 0.4417 - accuracy: 0.8491 - precision_1: 0.8743 - recall_1: 0.8225 - val_loss: 0.3370 - val_accuracy: 0.8901 - val_precision_1: 0.9054 - val_recall_1: 0.8779\n",
      "Epoch 4/10\n",
      "113/113 [==============================] - 7s 63ms/step - loss: 0.3613 - accuracy: 0.8807 - precision_1: 0.8967 - recall_1: 0.8673 - val_loss: 0.3205 - val_accuracy: 0.9042 - val_precision_1: 0.9146 - val_recall_1: 0.8968\n",
      "Epoch 5/10\n",
      "113/113 [==============================] - 7s 64ms/step - loss: 0.3232 - accuracy: 0.8967 - precision_1: 0.9080 - recall_1: 0.8860 - val_loss: 0.2971 - val_accuracy: 0.9058 - val_precision_1: 0.9184 - val_recall_1: 0.8920\n",
      "Epoch 6/10\n",
      "113/113 [==============================] - 7s 62ms/step - loss: 0.2853 - accuracy: 0.9121 - precision_1: 0.9234 - recall_1: 0.9025 - val_loss: 0.2642 - val_accuracy: 0.9206 - val_precision_1: 0.9308 - val_recall_1: 0.9123\n",
      "Epoch 7/10\n",
      "113/113 [==============================] - 7s 61ms/step - loss: 0.2670 - accuracy: 0.9172 - precision_1: 0.9267 - recall_1: 0.9101 - val_loss: 0.2472 - val_accuracy: 0.9302 - val_precision_1: 0.9379 - val_recall_1: 0.9249\n",
      "Epoch 8/10\n",
      "113/113 [==============================] - 7s 61ms/step - loss: 0.2433 - accuracy: 0.9283 - precision_1: 0.9363 - recall_1: 0.9215 - val_loss: 0.2426 - val_accuracy: 0.9289 - val_precision_1: 0.9361 - val_recall_1: 0.9224\n",
      "Epoch 9/10\n",
      "113/113 [==============================] - 7s 61ms/step - loss: 0.2437 - accuracy: 0.9265 - precision_1: 0.9351 - recall_1: 0.9193 - val_loss: 0.2460 - val_accuracy: 0.9269 - val_precision_1: 0.9361 - val_recall_1: 0.9205\n",
      "Epoch 10/10\n",
      "113/113 [==============================] - 7s 60ms/step - loss: 0.2202 - accuracy: 0.9343 - precision_1: 0.9428 - recall_1: 0.9281 - val_loss: 0.2274 - val_accuracy: 0.9389 - val_precision_1: 0.9453 - val_recall_1: 0.9351\n",
      "226/226 [==============================] - 5s 21ms/step - loss: 0.2274 - accuracy: 0.9389 - precision_1: 0.9453 - recall_1: 0.9351\n",
      "Model create_model_2\n",
      "Epoch 1/10\n",
      "169/169 [==============================] - 12s 61ms/step - loss: 0.8899 - accuracy: 0.6663 - precision_2: 0.7792 - recall_2: 0.5588 - val_loss: 0.4817 - val_accuracy: 0.8336 - val_precision_2: 0.8663 - val_recall_2: 0.8022\n",
      "Epoch 2/10\n",
      "169/169 [==============================] - 10s 57ms/step - loss: 0.4600 - accuracy: 0.8461 - precision_2: 0.8689 - recall_2: 0.8209 - val_loss: 0.3543 - val_accuracy: 0.8843 - val_precision_2: 0.8959 - val_recall_2: 0.8752\n",
      "Epoch 3/10\n",
      "169/169 [==============================] - 10s 58ms/step - loss: 0.3682 - accuracy: 0.8822 - precision_2: 0.8965 - recall_2: 0.8676 - val_loss: 0.3108 - val_accuracy: 0.9013 - val_precision_2: 0.9127 - val_recall_2: 0.8895\n",
      "Epoch 4/10\n",
      "169/169 [==============================] - 10s 56ms/step - loss: 0.3666 - accuracy: 0.8797 - precision_2: 0.8961 - recall_2: 0.8656 - val_loss: 0.2695 - val_accuracy: 0.9195 - val_precision_2: 0.9296 - val_recall_2: 0.9127\n",
      "Epoch 5/10\n",
      "169/169 [==============================] - 9s 56ms/step - loss: 0.3088 - accuracy: 0.9045 - precision_2: 0.9156 - recall_2: 0.8940 - val_loss: 0.2558 - val_accuracy: 0.9252 - val_precision_2: 0.9326 - val_recall_2: 0.9180\n",
      "Epoch 6/10\n",
      "169/169 [==============================] - 9s 56ms/step - loss: 0.2779 - accuracy: 0.9160 - precision_2: 0.9269 - recall_2: 0.9078 - val_loss: 0.2222 - val_accuracy: 0.9319 - val_precision_2: 0.9406 - val_recall_2: 0.9267\n",
      "Epoch 7/10\n",
      "169/169 [==============================] - 9s 56ms/step - loss: 0.2577 - accuracy: 0.9226 - precision_2: 0.9321 - recall_2: 0.9163 - val_loss: 0.2110 - val_accuracy: 0.9323 - val_precision_2: 0.9378 - val_recall_2: 0.9263\n",
      "Epoch 8/10\n",
      "169/169 [==============================] - 10s 58ms/step - loss: 0.2167 - accuracy: 0.9377 - precision_2: 0.9464 - recall_2: 0.9319 - val_loss: 0.1798 - val_accuracy: 0.9507 - val_precision_2: 0.9566 - val_recall_2: 0.9445\n",
      "Epoch 9/10\n",
      "169/169 [==============================] - 10s 62ms/step - loss: 0.2205 - accuracy: 0.9360 - precision_2: 0.9432 - recall_2: 0.9302 - val_loss: 0.1936 - val_accuracy: 0.9419 - val_precision_2: 0.9475 - val_recall_2: 0.9360\n",
      "Epoch 10/10\n",
      "169/169 [==============================] - 10s 60ms/step - loss: 0.1985 - accuracy: 0.9420 - precision_2: 0.9500 - recall_2: 0.9371 - val_loss: 0.1892 - val_accuracy: 0.9439 - val_precision_2: 0.9513 - val_recall_2: 0.9409\n",
      "226/226 [==============================] - 5s 23ms/step - loss: 0.1892 - accuracy: 0.9439 - precision_2: 0.9513 - recall_2: 0.9409\n",
      "Model create_model_2\n",
      "Epoch 1/10\n",
      "226/226 [==============================] - 16s 59ms/step - loss: 0.8066 - accuracy: 0.7055 - precision_3: 0.8064 - recall_3: 0.6079 - val_loss: 0.4039 - val_accuracy: 0.8664 - val_precision_3: 0.8999 - val_recall_3: 0.8371\n",
      "Epoch 2/10\n",
      "226/226 [==============================] - 12s 54ms/step - loss: 0.4142 - accuracy: 0.8632 - precision_3: 0.8821 - recall_3: 0.8430 - val_loss: 0.3358 - val_accuracy: 0.8900 - val_precision_3: 0.9023 - val_recall_3: 0.8780\n",
      "Epoch 3/10\n",
      "226/226 [==============================] - 13s 57ms/step - loss: 0.3461 - accuracy: 0.8902 - precision_3: 0.9030 - recall_3: 0.8768 - val_loss: 0.2607 - val_accuracy: 0.9227 - val_precision_3: 0.9323 - val_recall_3: 0.9137\n",
      "Epoch 4/10\n",
      "226/226 [==============================] - 13s 59ms/step - loss: 0.2978 - accuracy: 0.9086 - precision_3: 0.9203 - recall_3: 0.8996 - val_loss: 0.2887 - val_accuracy: 0.9087 - val_precision_3: 0.9176 - val_recall_3: 0.9013\n",
      "Epoch 5/10\n",
      "226/226 [==============================] - 14s 60ms/step - loss: 0.2658 - accuracy: 0.9195 - precision_3: 0.9285 - recall_3: 0.9107 - val_loss: 0.2737 - val_accuracy: 0.9155 - val_precision_3: 0.9261 - val_recall_3: 0.9066\n",
      "Epoch 6/10\n",
      "226/226 [==============================] - 13s 57ms/step - loss: 0.3219 - accuracy: 0.9026 - precision_3: 0.9159 - recall_3: 0.8904 - val_loss: 0.3062 - val_accuracy: 0.9090 - val_precision_3: 0.9193 - val_recall_3: 0.9024\n",
      "Epoch 7/10\n",
      "226/226 [==============================] - 13s 57ms/step - loss: 0.2658 - accuracy: 0.9214 - precision_3: 0.9308 - recall_3: 0.9134 - val_loss: 0.2300 - val_accuracy: 0.9371 - val_precision_3: 0.9444 - val_recall_3: 0.9292\n",
      "Epoch 8/10\n",
      "226/226 [==============================] - 13s 56ms/step - loss: 0.2307 - accuracy: 0.9345 - precision_3: 0.9429 - recall_3: 0.9286 - val_loss: 0.2178 - val_accuracy: 0.9337 - val_precision_3: 0.9436 - val_recall_3: 0.9280\n",
      "Epoch 9/10\n",
      "226/226 [==============================] - 12s 55ms/step - loss: 0.2399 - accuracy: 0.9284 - precision_3: 0.9370 - recall_3: 0.9215 - val_loss: 0.2504 - val_accuracy: 0.9258 - val_precision_3: 0.9328 - val_recall_3: 0.9206\n",
      "Epoch 10/10\n",
      "226/226 [==============================] - 13s 59ms/step - loss: 0.2200 - accuracy: 0.9383 - precision_3: 0.9460 - recall_3: 0.9324 - val_loss: 0.1869 - val_accuracy: 0.9473 - val_precision_3: 0.9540 - val_recall_3: 0.9434\n",
      "226/226 [==============================] - 5s 23ms/step - loss: 0.1869 - accuracy: 0.9473 - precision_3: 0.9540 - recall_3: 0.9434\n",
      "Model create_model_2\n",
      "Epoch 1/10\n",
      "282/282 [==============================] - 25s 80ms/step - loss: 0.7213 - accuracy: 0.7354 - precision_4: 0.8149 - recall_4: 0.6565 - val_loss: 0.4301 - val_accuracy: 0.8580 - val_precision_4: 0.8751 - val_recall_4: 0.8368\n",
      "Epoch 2/10\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.3915 - accuracy: 0.8709 - precision_4: 0.8871 - recall_4: 0.8540 - val_loss: 0.2983 - val_accuracy: 0.8987 - val_precision_4: 0.9134 - val_recall_4: 0.8875\n",
      "Epoch 3/10\n",
      "282/282 [==============================] - 17s 59ms/step - loss: 0.3112 - accuracy: 0.9012 - precision_4: 0.9131 - recall_4: 0.8918 - val_loss: 0.2383 - val_accuracy: 0.9284 - val_precision_4: 0.9383 - val_recall_4: 0.9202\n",
      "Epoch 4/10\n",
      "282/282 [==============================] - 19s 68ms/step - loss: 0.2573 - accuracy: 0.9234 - precision_4: 0.9318 - recall_4: 0.9164 - val_loss: 0.2319 - val_accuracy: 0.9339 - val_precision_4: 0.9435 - val_recall_4: 0.9291\n",
      "Epoch 5/10\n",
      "282/282 [==============================] - 19s 66ms/step - loss: 0.2222 - accuracy: 0.9348 - precision_4: 0.9424 - recall_4: 0.9288 - val_loss: 0.2211 - val_accuracy: 0.9317 - val_precision_4: 0.9383 - val_recall_4: 0.9266\n",
      "Epoch 6/10\n",
      "282/282 [==============================] - 17s 60ms/step - loss: 0.2075 - accuracy: 0.9398 - precision_4: 0.9472 - recall_4: 0.9343 - val_loss: 0.1961 - val_accuracy: 0.9459 - val_precision_4: 0.9519 - val_recall_4: 0.9413\n",
      "Epoch 7/10\n",
      " 11/282 [>.............................] - ETA: 22s - loss: 0.2138 - accuracy: 0.9403 - precision_4: 0.9468 - recall_4: 0.9361"
     ]
    }
   ],
   "source": [
    "best_model_history = None  # Keep track of the best model's history\n",
    "model_histories = []\n",
    "# Perform cross-validation\n",
    "models = [create_model_2]\n",
    "best_model = None\n",
    "num_folds = Parameters.number_folds\n",
    "tscv = TimeSeriesSplit(n_splits=num_folds)\n",
    "fold_acc_scores = []\n",
    "\n",
    "\n",
    "for i, (train, test) in enumerate(tscv.split(x_train)):\n",
    "    logging.info(f\"Fold {i+1}\")\n",
    "    train_x, train_y = x_train[train], y_train[train]\n",
    "    test_x, test_y = x_train[test], y_train[test]\n",
    "\n",
    "    fold_histories = []\n",
    "\n",
    "    for j, model_creator in enumerate(models):\n",
    "        print(f\"Model {model_creator.__name__}\")\n",
    "        model_name = f\"Model_{j+1}_Fold_{i+1}\"\n",
    "        model = model_creator(name=model_name)\n",
    "        logging.info(f\"Model {j+1}\")\n",
    "        history = model.fit(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            epochs=Parameters.epochs,\n",
    "            batch_size=Parameters.batch_size,\n",
    "            validation_data=(test_x, test_y),\n",
    "            verbose=Parameters.verbosity,\n",
    "        )\n",
    "        test_loss, acc, prec, recal = model.evaluate(\n",
    "            test_x, test_y, verbose=Parameters.verbosity\n",
    "        )\n",
    "        logging.info(f\"Validation accuracy: {acc}\")\n",
    "\n",
    "        fold_histories.append(history.history)\n",
    "\n",
    "        for epoch in range(Parameters.epochs):\n",
    "            # Log accuracy after each epoch\n",
    "            acc_epoch = history.history[\"val_accuracy\"][epoch]\n",
    "            logging.info(f\"Epoch {epoch + 1}, Validation accuracy: {acc_epoch}\")\n",
    "        fold_acc_scores.append((i, j, acc))\n",
    "\n",
    "        if best_model_history is None or acc > best_model_acc:\n",
    "            best_model_history = history\n",
    "            best_model = model  # Store the trained model instance\n",
    "            best_model_acc = acc\n",
    "\n",
    "    model_histories.append(fold_histories)\n",
    "\n",
    "# Find the best model\n",
    "best_model_index = np.argmax([score[2] for score in fold_acc_scores])\n",
    "best_fold_idx, best_model_idx, _ = max(fold_acc_scores, key=lambda x: x[2])\n",
    "best_model_history = model_histories[best_fold_idx][best_model_idx]\n",
    "print(best_model.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss und Accuracy Verlauf vom besten Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model.name)\n",
    "print(best_model_history[\"loss\"])\n",
    "# summarize history for accuracy\n",
    "plt.plot(best_model_history[\"accuracy\"])\n",
    "plt.plot(best_model_history[\"val_accuracy\"])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(best_model_history[\"loss\"])\n",
    "plt.plot(best_model_history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusions Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for multiclass classification\n",
    "y_pred = best_model.predict(x_validation)\n",
    "y_test_labels = y_validaton.argmax(axis=1)\n",
    "y_pred_labels = y_pred.argmax(axis=1)\n",
    "\n",
    "# create cm\n",
    "cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "display(le.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "# get f1 score of each class\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(y_test_labels, y_pred_labels, average=None)\n",
    "# get f1 score of each class\n",
    "\n",
    "# plot confusion matrix\n",
    "disp.plot()\n",
    "plt.show()\n",
    "display(f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Daten Predicten"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export bestes DL-Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export Model to json Tensorflow file\n",
    "import json\n",
    "\n",
    "best_model.save(\"saved_model/sensor_model.h5\")\n",
    "\n",
    "model = tf.keras.models.load_model(\"saved_model/sensor_model.h5\")\n",
    "\n",
    "# Save model architecture to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save model weights to a JSON file\n",
    "weights = model.get_weights()\n",
    "weights_as_list = [w.tolist() for w in weights]\n",
    "with open(\"weights.json\", \"w\") as f:\n",
    "    json.dump(weights_as_list, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell Einlesen und predicten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class validate_unseen_data():\n",
    "    def __init__(self, model_path=\"saved_model/sensor_model.h5\"):\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        self.csv_path = \"validation-velo-laufen-rennen.csv\"\n",
    "        \n",
    "    def predict_classes(self, file=\"06_iPhone12-2023-03-16_13-46-58Manjavy_KirupaVelofahren\", expected=''):\n",
    "        df_val = pd.read_csv(self.csv_path)\n",
    "        # Filter the rows where its velo in id_combined\n",
    "        df_val = df_val[df_val[\"id_combined\"].str.contains(file)]\n",
    "        df_val = df_val.drop(columns=[\"id\", \"user\", \"id_combined\"])\n",
    "        # convert the string time column to datetime\n",
    "        epoch = pd.Timestamp(\"1970-01-01\")\n",
    "        df_val[\"time\"] = pd.to_datetime(df_val[\"time\"])\n",
    "        df_val[\"time\"] = (df_val[\"time\"] - epoch).apply(\n",
    "            lambda x: int(x.total_seconds() * 1000)\n",
    "        )\n",
    "        df_val.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "        # get all types of the df\n",
    "        le = LabelEncoder()\n",
    "        df_val[\"class\"] = le.fit_transform(df_val[\"class\"])\n",
    "\n",
    "        # Set the window size and step size\n",
    "        window_size = 300\n",
    "        step_size = 100\n",
    "\n",
    "        # Reshape X to 2D format (samples, features)\n",
    "        X = df_val.values[:, 1:13]\n",
    "\n",
    "        # Create a sliding window of X with the specified window and step sizes\n",
    "        X_windows = np.array(\n",
    "            [\n",
    "                X[i : i + window_size, :]\n",
    "                for i in range(0, X.shape[0] - window_size + 1, step_size)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Reshape X_windows to 3D format (samples, timesteps, features)\n",
    "        timesteps = X_windows.shape[1]\n",
    "        n_features = X_windows.shape[2]\n",
    "        X_windows = X_windows.reshape(-1, timesteps, n_features)\n",
    "\n",
    "        y_pred_probs = model.predict(X_windows)\n",
    "\n",
    "        # Get the predicted class labels for each input window\n",
    "        y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "        # Print the predicted class labels\n",
    "        # get the median of the predicted labels\n",
    "        sol = np.median(y_pred_labels)\n",
    "\n",
    "        class_counts = np.bincount(y_pred_labels)\n",
    "        for i, count in enumerate(class_counts):\n",
    "            print(f\"Class {i} count: {count}\")\n",
    "        \n",
    "        return (sol, expected)\n",
    "\n",
    "median = validate_unseen_data().predict_classes('06_iPhone12-2023-03-16_13-46-58Manjavy_KirupaVelofahren', expected='velo')\n",
    "median2 = validate_unseen_data().predict_classes('01_iPhone13ProMax-2023-03-15_18-29-42Gabriel_TorresRennen', expected='rennen')\n",
    "median3 = validate_unseen_data().predict_classes('01_iPhone13pro-2023-03-21_16-55-47Etienne_RouletLaufen', expected='laufen')\n",
    "\n",
    "(median, median2, median3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Conclusion is that the model is not overfitted but Gabriel is more a Stepper than a Runner ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
