{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensor Based Activity Recoginition \n",
    "Challenge: cdl1 - Sensor based Activity Recognition  \n",
    "Team: Lea Bütler, Manjavy Kirupa, Etienne Roulet, Si Ben Tran  \n",
    "\n",
    "Aufgabe: DL Modell erstellen\n",
    "\n",
    "Hier in diesem Notebook erstellen wir unsere Deep Learning Modelle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Importieren & GPU Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn Libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn.model_selection import KFold, train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# keras Libraries\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Tensorflow Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# dataclass\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# general setup\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "# datetime as filename for logging\n",
    "now = datetime.now()\n",
    "date_time_string = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, filename=f\"{date_time_string}.txt\", filemode=\"a\"\n",
    ")\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "# Num of Gpus\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen\n",
    "\n",
    "- Einlesen der **Getrimmten Daten**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Parameters\n",
    "@dataclass\n",
    "class Parameters:\n",
    "    batch_size: int = 64\n",
    "    epochs: int = 20\n",
    "    verbosity: str = \"auto\"\n",
    "    number_folds: int = 6\n",
    "    output_size: int = 6\n",
    "    window_size = 400\n",
    "    step_size = 10\n",
    "    data_augmentation  = False\n",
    "    \n",
    "\n",
    "# Getrimmte Sensordaten einlesen\n",
    "df = pd.read_csv(\"../Sensor_Data-Wrangling-und-EDA/Alle_Messungen_trimmed.csv\", index_col=0)\n",
    "display(df)\n",
    "\n",
    "# convert the string time column to datetime\n",
    "epoch = pd.Timestamp(\"1970-01-01\")\n",
    "\n",
    "# Time column into datetime\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "\n",
    "# Time column into milliseconds\n",
    "df[\"time\"] = (df[\"time\"] - epoch).apply(lambda x: int(x.total_seconds() * 1000))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    "\n",
    "- Beim Preprocessing entfernen von drei zufällige Sensor Messungen aus unserem Datensatz  \n",
    "- Validierungsdatensatz daraus erstellen, der verwendet wird, um das Modell zu validieren\n",
    "- Nicht relevante Spalten löschen\n",
    "- Funktion für *augment_data* erstellen, um Daten zu erweitern\n",
    "- Transformation der Daten in passende Window Size mit Step Size für das DL-Modell\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select three random id_combines files\n",
    "ids = [\"06_iPhone12-2023-03-16_13-46-58Manjavy_KirupaVelofahren\", \n",
    "       \"01_iPhone13pro-2023-03-21_16-55-47Etienne_RouletLaufen\", \n",
    "       \"01_iPhone13ProMax-2023-03-15_18-29-42Gabriel_TorresRennen\",\n",
    "       \"03_Huawei_Stehen-2023-03-16_15-27-54Lea_BuetlerStehen\",\n",
    "       \"01_iPhone14Pro_2023-02-28_20-09-53Yvo_KellerTreppenlaufen\",\n",
    "       \"01_iPhone14-2023-02-27-2023-02-27_07-39-23Ognjen_ColovicSitzen\"]\n",
    "\n",
    "# create a dataframe with these three random files\n",
    "df_validation = df[df[\"id_combined\"].isin(ids)]\n",
    "\n",
    "# export the dataframe as a csv file\n",
    "df_validation.to_csv(\"data/validation-data.csv\", index=False)\n",
    "\n",
    "# remove the validation data from the dataframe \n",
    "df = df[~df[\"id_combined\"].isin(ids)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unnötige Spalten und Class Encoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns that are not needed\n",
    "df = df.drop(columns=[\"id\", \"user\", \"id_combined\"])\n",
    "# get all types of the df\n",
    "le = LabelEncoder()\n",
    "df[\"class\"] = le.fit_transform(df[\"class\"])\n",
    "# print dictionary of the classes and its encoded values\n",
    "print(dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten Transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion `add_noise` fügt dem gegebenen Zeitreihen-Datensatz Rauschen hinzu. Sie generiert eine zufällige Rauschverteilung mit der gleichen Länge wie die Zeitreihe und multipliziert sie mit dem Rauschfaktor. Das resultierende Rauschen wird dann zur Zeitreihe addiert, um den rauschigen Datensatz zu erhalten.\n",
    "\n",
    "Die Funktion `flip_time_series` kehrt die Reihenfolge der Werte in der Zeitreihe um. Dies kann nützlich sein, um bestimmte Muster oder Trends in den Daten zu erkennen.\n",
    "\n",
    "Die Funktion `scale_magnitude` skaliert die Amplitude der Zeitreihe um den gegebenen Skalierungsfaktor. Dies ändert die Höhe der Werte in der Zeitreihe, ohne die Form oder das Muster der Daten zu beeinflussen.\n",
    "\n",
    "Die Funktion `augment_data` verwendet die oben genannten Funktionen, um die gegebenen Eingangsdaten (X) und die zugehörigen Labels (y) zu erweitern. Sie erstellt leere Listen für die erweiterten Daten und Labels. Für jeden Datensatz in X werden folgende Schritte durchgeführt:\n",
    "\n",
    "1. Rauschen hinzufügen: Die Funktion `add_noise` wird auf den Datensatz angewendet, wobei der Rauschfaktor übergeben wird. Der rauschige Datensatz und das entsprechende Label werden den erweiterten Listen hinzugefügt.\n",
    "\n",
    "2. Amplitude skalieren: Die Funktion `scale_magnitude` wird auf den Datensatz angewendet, wobei der Skalierungsfaktor übergeben wird. Der skalierte Datensatz und das entsprechende Label werden den erweiterten Listen hinzugefügt.\n",
    "\n",
    "3. Zeitreihe umkehren: Falls eine zufällig generierte Zahl kleiner als die angegebene Flipp-Wahrscheinlichkeit ist, wird die Funktion `flip_time_series` auf den Datensatz angewendet. Der umgekehrte Datensatz und das entsprechende Label werden den erweiterten Listen hinzugefügt.\n",
    "\n",
    "Am Ende werden die erweiterten Listen in Numpy-Arrays umgewandelt und als Ergebnis zurückgegeben.\n",
    "\n",
    "Zusammenfassend führt die Funktion `augment_data` eine Datenaugmentation für Zeitreihen durch, indem sie Rauschen hinzufügt, die Amplitude skaliert und/oder die Zeitreihen umkehrt. Dies kann helfen, die Datenvielfalt zu erhöhen und das Modelltraining robuster zu machen.\n",
    "\n",
    "Die Zeit zum trainieren des Modells wird durch die augementierung der Daten mindestens verdreifacht, darum nur mit bedacht augumentieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to add noise to time series\n",
    "def add_noise(time_series, noise_factor):\n",
    "    noise = np.random.randn(len(time_series)) * noise_factor\n",
    "    return time_series + noise\n",
    "\n",
    "# Flip time series\n",
    "def flip_time_series(time_series):\n",
    "    return np.flip(time_series)\n",
    "\n",
    "# Scale magnitude\n",
    "def scale_magnitude(time_series, scaling_factor):\n",
    "    return time_series * scaling_factor\n",
    "\n",
    "# Augment the data\n",
    "def augment_data(X, y, noise_factor, scaling_factor, flip_probability):\n",
    "    augmented_X = []\n",
    "    augmented_y = []\n",
    "    for i in range(X.shape[0]):\n",
    "        x_sample = X[i]\n",
    "        y_sample = y[i]\n",
    "\n",
    "        # Add noise\n",
    "        x_sample_noisy = add_noise(x_sample, noise_factor)\n",
    "        augmented_X.append(x_sample_noisy)\n",
    "        augmented_y.append(y_sample)\n",
    "\n",
    "        # Scale magnitude\n",
    "        x_sample_scaled = scale_magnitude(x_sample, scaling_factor)\n",
    "        augmented_X.append(x_sample_scaled)\n",
    "        augmented_y.append(y_sample)\n",
    "\n",
    "        # Flip time series\n",
    "        if random.random() < flip_probability:\n",
    "            x_sample_flipped = flip_time_series(x_sample)\n",
    "            augmented_X.append(x_sample_flipped)\n",
    "            augmented_y.append(y_sample)\n",
    "\n",
    "    return np.array(augmented_X), np.array(augmented_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benutzerdefinierte Daten-Generator-Klasse namens `DataGenerator`, die das `Sequence`-Modul aus `tf.keras.utils` verwendet. Diese Klasse ermöglicht es, Daten in Batches zu laden und sie für das Training von Modellen in TensorFlow effizient zu verwenden.\n",
    "\n",
    "Die Klasse `DataGenerator` hat die folgenden wichtigen Methoden:\n",
    "\n",
    "1. `__init__(self, x_data, y_data, batch_size)`: Der Initialisierungsmethode werden die Eingangsdaten `x_data`, die zugehörigen Labels `y_data` und die gewünschte Batch-Größe `batch_size` übergeben. Diese Daten werden in den Attributen der Klasse gespeichert.\n",
    "\n",
    "2. `__len__(self)`: Diese Methode gibt die Anzahl der Batches zurück, die der Generator erzeugen kann. Sie berechnet die Länge basierend auf der Gesamtanzahl der Eingangsdaten und der gewählten Batch-Größe. Der Rückgabewert wird durch Aufrunden mit `np.ceil` auf die nächste Ganzzahl aufgerundet.\n",
    "\n",
    "3. `__getitem__(self, idx)`: Diese Methode wird aufgerufen, um einen bestimmten Batch an Eingangsdaten und Labels zurückzugeben, der durch den Index `idx` identifiziert wird. Sie extrahiert den entsprechenden Teil der Eingangsdaten und Labels basierend auf dem Index und der Batch-Größe. Der zurückgegebene Batch besteht aus `batch_x` (Eingangsdaten) und `batch_y` (Labels).\n",
    "\n",
    "Der Grund, warum dieser Daten-Generator für die GPU nützlich ist, liegt darin, dass er ermöglicht, die Daten in Batches zu laden und sie parallel auf der GPU zu verarbeiten. GPUs sind effizienter bei der Verarbeitung von großen Mengen von Daten, insbesondere wenn sie in Batches parallel verarbeitet werden können. Durch die Verwendung eines Daten-Generators wie `DataGenerator` können die Daten während des Trainings in Batches geladen werden, wodurch die Nutzung der GPU-Ressourcen optimiert und die Trainingsgeschwindigkeit verbessert wird. Dies ist besonders wichtig, wenn der verfügbare Arbeitsspeicher begrenzt ist und nicht alle Daten auf einmal in den Speicher passen. Der Daten-Generator lädt jeweils nur den benötigten Batch in den Speicher, während der vorherige Batch auf der GPU verarbeitet wird. Dadurch kann eine kontinuierliche Verarbeitung der Daten auf der GPU erreicht werden, was die Trainingseffizienz erhöht.\n",
    "\n",
    "Wurde im lauf der Challenge hinzugefügt, weil der Speicherplatz auf der RTX 3080 nicht mehr reichte (auch mit sehr niederigen batch Sizen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom data generator using tf.keras.utils.Sequence\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_data, y_data, batch_size):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x_data) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x_data[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        batch_y = self.y_data[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Festlegen der Fenstergröße und Schrittgröße:\n",
    "   - `window_size`: Die Größe der Fenster, die in den Daten verwendet werden sollen. Dies gibt an, wie viele aufeinanderfolgende Zeitpunkte in einem Fenster enthalten sein sollen.\n",
    "   - `step_size`: Der Schritt, um das Fenster bei jedem Schritt zu verschieben. Es gibt an, wie viele Zeitpunkte das Fenster bei jedem Schritt vorrückt.\n",
    "\n",
    "2. Auswahl von Merkmalen und Labels:\n",
    "   - `X`: Eine Matrix der Merkmale, die aus dem Datenframe extrahiert werden. Hier werden die Spalten 1 bis 12 des Datenframes verwendet.\n",
    "   - `y`: Ein eindimensionaler Vektor mit den zugehörigen Labels, die aus der Spalte \"class\" des Datenframes stammen.\n",
    "\n",
    "3. Erzeugung leerer Listen für Fenster und Labels:\n",
    "   - `X_windows`: Eine leere Liste, die die Fensterdaten aufnehmen wird.\n",
    "   - `y_windows`: Eine leere Liste, die die zugehörigen Labels für die Fensterdaten aufnehmen wird.\n",
    "\n",
    "4. Datenaugmentation (falls erforderlich):\n",
    "   - Überprüfung der Variable `Parameters.data_augmentation`, um festzustellen, ob eine Datenaugmentation durchgeführt werden soll.\n",
    "   - Falls ja, werden Rausch-, Skalierungs- und Umkehrfaktoren festgelegt.\n",
    "   - Die Funktion `augment_data` wird auf die Daten `X` und die Labels `y` angewendet, um die Daten zu erweitern.\n",
    "   - Die erweiterten Daten und Labels werden den Variablen `X_augmented` und `y_augmented` zugewiesen.\n",
    "\n",
    "5. Erzeugung von Datenfenstern:\n",
    "   - Schleife, um für jedes Fenster Daten und Labels zu erstellen.\n",
    "   - Wenn Datenaugmentation durchgeführt wurde:\n",
    "     - Das Fenster wird aus den erweiterten Daten `X_augmented` und `y_augmented` extrahiert.\n",
    "     - Überprüfung, ob es innerhalb des Fensters einen Aktivitätsübergang gibt.\n",
    "     - Wenn kein Aktivitätsübergang vorliegt, werden das Fenster (`window_data`) und das zugehörige Label (`window_labels[0]`) zu den Listen `X_windows` und `y_windows` hinzugefügt.\n",
    "   - Wenn keine Datenaugmentation durchgeführt wurde:\n",
    "     - Das Fenster wird aus den ursprünglichen Daten `X` und `y` extrahiert.\n",
    "     - Überprüfung, ob es innerhalb des Fensters einen Aktivitätsübergang gibt.\n",
    "     - Wenn kein Aktivitätsübergang vorliegt, werden das Fenster (`window_data`) und das zugehörige Label (`window_labels[0]`) zu den Listen `X_windows` und `y_windows` hinzugefügt.\n",
    "\n",
    "6. Aktualisierung der Datenformate:\n",
    "   - Die Listen `X_windows` und `y_windows` werden in Numpy-Arrays umgewandelt.\n",
    "   - Die Dimensionen der Daten werden entsprechend den Anforderungen des Modells angepasst.\n",
    "   - Die Anzahl der Zeitpunkte pro Fenster (`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the window size and step size parameters\n",
    "window_size = Parameters.window_size\n",
    "step_size = Parameters.step_size\n",
    "\n",
    "# select the features and labels\n",
    "X = df.values[:, 1:13]\n",
    "y = df[\"class\"].values\n",
    "\n",
    "# emtpy lists for the windows and labels\n",
    "X_windows = []\n",
    "y_windows = []\n",
    "\n",
    "# create if statement for data augmentation\n",
    "if Parameters.data_augmentation:\n",
    "    noise_factor = 0.1\n",
    "    scaling_factor = 1.5\n",
    "    flip_probability = 0.5\n",
    "\n",
    "    # Augment the data\n",
    "    X_augmented, y_augmented = augment_data(X, y, noise_factor, scaling_factor, flip_probability)\n",
    "    \n",
    "    augmented_i = 0\n",
    "    i = 0\n",
    "    while augmented_i + window_size <= X_augmented.shape[0]:\n",
    "        window_data = X_augmented[augmented_i : augmented_i + window_size, :]\n",
    "        window_labels = y_augmented[augmented_i : augmented_i + window_size]\n",
    "        \n",
    "        # Check if there is an activity transition within the window\n",
    "        if len(np.unique(window_labels)) == 1:\n",
    "            X_windows.append(window_data)\n",
    "            y_windows.append(window_labels[0])\n",
    "            \n",
    "        augmented_i += step_size\n",
    "        i += step_size\n",
    "# if no data augmentation is needed\n",
    "else:\n",
    "    i = 0\n",
    "    while i + window_size <= X.shape[0]:\n",
    "        window_data = X[i : i + window_size, :]\n",
    "        window_labels = y[i : i + window_size]\n",
    "        \n",
    "        # Check if there is an activity transition within the window\n",
    "        if len(np.unique(window_labels)) == 1:\n",
    "            X_windows.append(window_data)\n",
    "            y_windows.append(window_labels[0])\n",
    "            \n",
    "        i += step_size\n",
    "\n",
    "X_windows = np.array(X_windows)\n",
    "# Update timesteps value\n",
    "timesteps = X_windows.shape[1]  \n",
    "n_features = X_windows.shape[2]\n",
    "X_windows = X_windows.reshape(-1, timesteps, n_features)\n",
    "y_windows = to_categorical(y_windows, num_classes=6)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(\n",
    "    X_windows, y_windows, test_size=0.2, random_state=42, stratify=y_windows\n",
    ")\n",
    "\n",
    "# Create data generators\n",
    "train_generator = DataGenerator(x_train, y_train, batch_size=Parameters.batch_size)\n",
    "validation_generator = DataGenerator(x_validation, y_validation, batch_size=Parameters.batch_size)\n",
    "\n",
    "# print Data shapes\n",
    "print(\"Data shapes:\")\n",
    "print(\"- x:         = {}\".format(X.shape))\n",
    "print(\"- y:         = {}\".format(y.shape))\n",
    "print(\"- x_train:   = {}\".format(x_train.shape))\n",
    "print(\"- y_train:   = {}\".format(y_train.shape))\n",
    "print(\"- x_validation:    = {}\".format(x_validation.shape))\n",
    "print(\"- y_validation:    = {}\".format(y_validation.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL-Modelle erstellen\n",
    "\n",
    "- DL Modelle mit Tensorflow Framework erstellt \n",
    "\n",
    "Tabellarische Auflistung aller DL-Modelle und deren Parameter\n",
    "\n",
    "| DL-Modell | Anzahl CNN Layer | CNN Filter Size | Kernel Size | Aktivierungsfunktion | L2-Regularisierung | Dropout | Anzahl LSTM Layer | LSTM Neuronen | Anzahl Dense Layer | Dense Neuronen | \n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| 01 | 1 | 64 | 2 | relu | 0.01 | - | 1 | 100 | 1 | 6 | \n",
    "| 02 | 1 | 64 | 4 | relu | 0.01 | 0.2 | 1 | 100 | 1 | 6 |  \n",
    "| 03 | 1 | 32 | 2 | relu | 0.01 | - | 1 | 100 | 1 | 6 | \n",
    "| 04 | 1 | 64 | 2 | relu | 0.01 | - | 1 | 100 | 1 | 6 | \n",
    "| 05 | 3 | (32,20,10) | 2 | relu | 0.01 | - | - | - | 3 | (180,100,6) | \n",
    "| 06 | 2 | (64,32) | 2 | relu | 0.01 | - | - | - | 4 | (200,100,50,6) | \n",
    "| 07 | 5 | (64,64,64,64,64) | 2 | relu | 0.05 | - | - | - | 6 | (200,150,100,50,25,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Deep Learning Models\n",
    "\n",
    "# First Deep Learning Model - CNN - LSTM\n",
    "def create_model_1(name=\"model_1\"):\n",
    "    '''\n",
    "    CNN Model with 1 Convolutional Layer, 1 LSTM Layer and 1 Dense Layer \n",
    "    '''\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=64,\n",
    "                kernel_size=12,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.LSTM(100),\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Second Deep Learning Model - CNN - LSTM\n",
    "def create_model_2(name=\"model_2\"):\n",
    "    '''\n",
    "    CNN Model with one Conv1D layer, one LSTM layer and one Dense layer, also using l2 regularization and dropout\n",
    "    '''\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=16,\n",
    "                kernel_size=4,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100)),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Thrid Deep Learning Model - CNN - LSTM\n",
    "def create_model_3(name=\"model_3\"):\n",
    "    '''\n",
    "    CNN Model with 32 Filters combine with LSTM and one dense layer\n",
    "    '''\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=32,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.LSTM(100),\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Fourth Deep Learning Model - CNN - LSTM\n",
    "def create_model_4(name=\"model_4\"):\n",
    "    '''\n",
    "    CNN Model with 64 Filters combined with LSTM and one dense layer\n",
    "    '''\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=64,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.LSTM(100),\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Fifth Deep Learning Model - CNN - MLP\n",
    "def create_model_5(name=\"model_5\"):\n",
    "    '''\n",
    "    CNN Model with 3 Conv1D Layers and 3 Dense Layers with Batch Normalization and L2 Regularization\n",
    "    '''\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # First Conv1D layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=32,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Batch Normalization and MaxPooling \n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Second Conv1D layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=20, \n",
    "                kernel_size=2, \n",
    "                activation=\"relu\", \n",
    "                padding=\"same\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Batch Normalization and MaxPooling\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Third Conv1D layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=10, \n",
    "                kernel_size=2, \n",
    "                activation=\"relu\", \n",
    "                padding=\"same\", \n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Batch Normalization and MaxPooling\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Flatten output\n",
    "            tf.keras.layers.Flatten(),\n",
    "            # Add first dense layer\n",
    "            tf.keras.layers.Dense(units = 180, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(0.01),),\n",
    "            # Batch Normalization\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add second dense layer\n",
    "            tf.keras.layers.Dense(units = 100, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(0.01),),\n",
    "            # Batch Normalization\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add third dense layer - Output Layer with softmax activation\n",
    "            tf.keras.layers.Dense(6, activation=\"softmax\"),\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Sixth Deep Learning Model - CNN - MLP\n",
    "def create_model_6(name=\"model_6\", \n",
    "                   filterconv1 = 64, \n",
    "                   filterconv2 = 32, \n",
    "                   unitsdense1 = 200, \n",
    "                   unitsdense2 = 100, \n",
    "                   unitsdense3 = 50, \n",
    "                   l2_reg = 0.01):\n",
    "    '''\n",
    "    CNN Model with 2 Conv1D Layers and 4 Dense Layers with Batch Normalization and L2 Regularization\n",
    "    '''\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # First Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=filterconv1,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Second Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=filterconv2, \n",
    "                kernel_size=2, \n",
    "                activation=\"relu\", \n",
    "                padding=\"same\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Flatten the results\n",
    "            tf.keras.layers.Flatten(),\n",
    "            # Add first dense layer\n",
    "            tf.keras.layers.Dense(units = unitsdense1, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add second dense layer\n",
    "            tf.keras.layers.Dense(units = unitsdense2, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add third dense layer\n",
    "            tf.keras.layers.Dense(units = unitsdense3, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add output Layer with Softmax Activation Funktion\n",
    "            tf.keras.layers.Dense(units = 6, \n",
    "                                  activation=\"softmax\"),\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    # Define optimizer, loss and metrics \n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Seventh Deep Learning Model - CNN - MLP\n",
    "def create_model_7(name=\"model_7\", \n",
    "                   filterconv1 = 64, \n",
    "                   filterconv2 = 64, \n",
    "                   filterconv3 = 64, \n",
    "                   filterconv4 = 64,\n",
    "                   filterconv5 = 64, \n",
    "                   unitsdense1 = 200, \n",
    "                   unitsdense2 = 150, \n",
    "                   unitsdense3 = 100,\n",
    "                   unitsdense4 = 50,\n",
    "                   unitsdense5 = 25,\n",
    "                   l2_reg = 0.05):\n",
    "    '''\n",
    "    Check, if the deeper the model the better the results? \n",
    "    CNN Model with 5 Conv1D Layers and 6 Dense Layers with Batch Normalization and L2 Regularization\n",
    "    '''\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # First Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=filterconv1,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            ),\n",
    "            # Batchnormalization and Maxpooling\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Second Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=filterconv2, \n",
    "                kernel_size=2, \n",
    "                activation=\"relu\", \n",
    "                padding=\"same\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            ),\n",
    "            # Batchnormalization and Maxpooling\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Third Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=filterconv3, \n",
    "                kernel_size=2, \n",
    "                activation=\"relu\", \n",
    "                padding=\"same\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            ),\n",
    "            # Batchnormalization and Maxpooling\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Fourth Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=filterconv4,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            ),\n",
    "            # Batchnormalization and Maxpooling\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Fifth Conv1D Layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=filterconv5,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(l2_reg),\n",
    "            ),\n",
    "            # Batchnormalization and Maxpooling\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # Flatten the results\n",
    "            tf.keras.layers.Flatten(),\n",
    "            # Add first dense layer\n",
    "            tf.keras.layers.Dense(units = unitsdense1, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add second dense layer\n",
    "            tf.keras.layers.Dense(units = unitsdense2, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add third dense layer\n",
    "            tf.keras.layers.Dense(units = unitsdense3, \n",
    "                                  activation=\"relu\", \n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Add fourth dense layer\n",
    "            tf.keras.layers.Dense(units = unitsdense4,\n",
    "                                  activation=\"relu\",\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # add fifth dense layer\n",
    "            tf.keras.layers.Dense(units = unitsdense5,\n",
    "                                  activation=\"relu\",\n",
    "                                  kernel_regularizer=tf.keras.regularizers.l2(l2_reg),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "        \n",
    "            # Add output Layer with Softmax Activation Funktion\n",
    "            tf.keras.layers.Dense(units = 6, \n",
    "                                  activation=\"softmax\"),\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    # Define optimizer, loss and metrics \n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Baseline Model\n",
    "def create_baseline_model(name=\"baseline_model\"):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.layers.Flatten(input_shape=(timesteps, n_features)),\n",
    "            tf.keras.layers.Dense(6, activation=\"softmax\")\n",
    "        ],\n",
    "        name=name\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL-Modelle Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m model \u001b[39m=\u001b[39m model_creator(name\u001b[39m=\u001b[39mmodel_name)\n\u001b[0;32m     15\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel \u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     17\u001b[0m     train_generator,\n\u001b[0;32m     18\u001b[0m     epochs\u001b[39m=\u001b[39;49mParameters\u001b[39m.\u001b[39;49mepochs,\n\u001b[0;32m     19\u001b[0m     batch_size\u001b[39m=\u001b[39;49mParameters\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[0;32m     20\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_generator,\n\u001b[0;32m     21\u001b[0m     verbose\u001b[39m=\u001b[39;49mParameters\u001b[39m.\u001b[39;49mverbosity,\n\u001b[0;32m     22\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[TensorBoard(log_dir\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlogs/\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)],\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m test_loss, acc, prec, recal \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(\n\u001b[0;32m     25\u001b[0m     x_validation, y_validation, verbose\u001b[39m=\u001b[39mParameters\u001b[39m.\u001b[39mverbosity\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValidation accuracy: \u001b[39m\u001b[39m{\u001b[39;00macc\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\super\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\super\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:1445\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1431\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_eval_data_handler\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1432\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_eval_data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39mget_data_handler(\n\u001b[0;32m   1433\u001b[0m       x\u001b[39m=\u001b[39mval_x,\n\u001b[0;32m   1434\u001b[0m       y\u001b[39m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1443\u001b[0m       model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[0;32m   1444\u001b[0m       steps_per_execution\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution)\n\u001b[1;32m-> 1445\u001b[0m val_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate(\n\u001b[0;32m   1446\u001b[0m     x\u001b[39m=\u001b[39;49mval_x,\n\u001b[0;32m   1447\u001b[0m     y\u001b[39m=\u001b[39;49mval_y,\n\u001b[0;32m   1448\u001b[0m     sample_weight\u001b[39m=\u001b[39;49mval_sample_weight,\n\u001b[0;32m   1449\u001b[0m     batch_size\u001b[39m=\u001b[39;49mvalidation_batch_size \u001b[39mor\u001b[39;49;00m batch_size,\n\u001b[0;32m   1450\u001b[0m     steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[0;32m   1451\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m   1452\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1453\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1454\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1455\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1456\u001b[0m     _use_cached_eval_dataset\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m   1457\u001b[0m val_logs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m name: val \u001b[39mfor\u001b[39;00m name, val \u001b[39min\u001b[39;00m val_logs\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m   1458\u001b[0m epoch_logs\u001b[39m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Users\\super\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\super\\miniconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py:1756\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1754\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m, step_num\u001b[39m=\u001b[39mstep, _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1755\u001b[0m   callbacks\u001b[39m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1756\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_function(iterator)\n\u001b[0;32m   1757\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1758\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Keep track of the best model's history\n",
    "best_model_history = None  \n",
    "model_histories = []\n",
    "\n",
    "# Perform training and validation\n",
    "models = [create_baseline_model, create_model_1,create_model_2,create_model_3,create_model_4,create_model_5,create_model_6,create_model_7]\n",
    "best_model = None\n",
    "best_model_acc = 0\n",
    "\n",
    "# start training\n",
    "for j, model_creator in enumerate(models):\n",
    "    print(f\"Model {model_creator.__name__}\")\n",
    "    model_name = f\"Model_{j+1}\"\n",
    "    model = model_creator(name=model_name)\n",
    "    logging.info(f\"Model {j+1}\")\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=Parameters.epochs,\n",
    "        batch_size=Parameters.batch_size,\n",
    "        validation_data=validation_generator,\n",
    "        verbose=Parameters.verbosity,\n",
    "        callbacks=[TensorBoard(log_dir=f\"logs/{model_name}\")],\n",
    "    )\n",
    "    test_loss, acc, prec, recal = model.evaluate(\n",
    "        x_validation, y_validation, verbose=Parameters.verbosity\n",
    "    )\n",
    "    logging.info(f\"Validation accuracy: {acc}\")\n",
    "\n",
    "    model_histories.append(history.history)\n",
    "\n",
    "    for epoch in range(Parameters.epochs):\n",
    "        # Log accuracy after each epoch\n",
    "        acc_epoch = history.history[\"val_accuracy\"][epoch]\n",
    "        logging.info(f\"Epoch {epoch + 1}, Validation accuracy: {acc_epoch}\")\n",
    "\n",
    "    if best_model_history is None or acc > best_model_acc:\n",
    "        best_model_history = history\n",
    "        # Store the trained model instance\n",
    "        best_model = model  \n",
    "        best_model_acc = acc\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(f'saved_model/sensor_model/{model_creator.__name__}.h5')\n",
    "\n",
    "print(best_model.name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alle Modell Einlesen und predicten um bestes Modell anhand unseen Data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hier werden alle Modelle die Trainiert wurden anhand von komplett neuen Daten bewerted und evaluiert.\n",
    "- Dies ist wichtig, da wir ein generelles Modell erzeugen möchten und keines das Overfitted auf nur den gegebenen Daten\n",
    "- Dazu lädt der Code hier alle Modelle aus dem Folder: saved_model/sensor_model/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class for validation data of unseen data\n",
    "# import label encoder as le\n",
    "class validate_unseen_data():\n",
    "    def __init__(self, model_path=\"saved_model/sensor_model.h5\"):\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        self.csv_path = \"data/validation-data.csv\"\n",
    "        \n",
    "    \n",
    "    def predict_classes(self, file=\"06_iPhone12-2023-03-16_13-46-58Manjavy_KirupaVelofahren\", expected=1.0):\n",
    "        df_val = pd.read_csv(self.csv_path)\n",
    "        # Filter the rows where its velo in id_combined\n",
    "        df_val = df_val[df_val[\"id_combined\"].str.contains(file)]\n",
    "        df_val = df_val.drop(columns=[\"id\", \"user\", \"id_combined\"])\n",
    "        # convert the string time column to datetime\n",
    "        epoch = pd.Timestamp(\"1970-01-01\")\n",
    "        df_val[\"time\"] = pd.to_datetime(df_val[\"time\"])\n",
    "        df_val[\"time\"] = (df_val[\"time\"] - epoch).apply(\n",
    "            lambda x: int(x.total_seconds() * 1000)\n",
    "        )\n",
    "        # get all types of the df\n",
    "        le = LabelEncoder()\n",
    "        df_val[\"class\"] = le.fit_transform(df_val[\"class\"])\n",
    "    \n",
    "        # Set the window size and step size\n",
    "        window_size = 400\n",
    "        step_size = 10\n",
    "\n",
    "        # Reshape X to 2D format (samples, features)\n",
    "        X = df_val.values[:, 1:13]\n",
    "        # X to textfile\n",
    "\n",
    "        # print X in a loop and in a valid Python Array Format and comma seperated array values\n",
    "        \n",
    "        \n",
    "        # write textfile with the X data in the following format [X1\n",
    "\n",
    "        # Create a sliding window of X with the specified window and step sizes\n",
    "        X_windows = np.array(\n",
    "            [\n",
    "                X[i : i + window_size, :]\n",
    "                for i in range(0, X.shape[0] - window_size + 1, step_size)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Reshape X_windows to 3D format (samples, timesteps, features)\n",
    "        timesteps = X_windows.shape[1]\n",
    "        n_features = X_windows.shape[2]\n",
    "        X_windows = X_windows.reshape(-1, timesteps, n_features)\n",
    "\n",
    "        y_pred_probs = self.model.predict(X_windows)\n",
    "\n",
    "        # Get the predicted class labels for each input window\n",
    "        y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "        # Print the predicted class labels\n",
    "        # get the median of the predicted labels\n",
    "        sol = np.median(y_pred_labels)\n",
    "        # caluclate the most frequent class not the median\n",
    "        sol = np.bincount(y_pred_labels).argmax()\n",
    "        print(f'has classified: {sol}')\n",
    "        print(f'was {expected}')\n",
    "\n",
    "        class_counts = np.bincount(y_pred_labels)\n",
    "        for i, count in enumerate(class_counts):\n",
    "            print(f\"Class {i} count: {count}\")\n",
    "        \n",
    "        return (sol, expected)\n",
    "\n",
    "import glob\n",
    "\n",
    "path = \"saved_model/sensor_model/*.h5\"\n",
    "h5_files = glob.glob(path)\n",
    "print(h5_files)\n",
    "\n",
    "best_model = None\n",
    "lowest_difference = float('inf')\n",
    "most_correct = 0\n",
    "\n",
    "\n",
    "for h5_file in h5_files:\n",
    "    model_name = h5_file.split(\"/\")[-1]\n",
    "    print(f\"Model name: {model_name}\")\n",
    "    \n",
    "    # Load the model\n",
    "    model = validate_unseen_data(h5_file)\n",
    "    \n",
    "    # Calculate the differences\n",
    "    median1 = model.predict_classes('06_iPhone12-2023-03-16_13-46-58Manjavy_KirupaVelofahren', 5)[0]\n",
    "    median2 = model.predict_classes('01_iPhone13pro-2023-03-21_16-55-47Etienne_RouletLaufen', 0)[0]\n",
    "    median3 = model.predict_classes(\"01_iPhone14-2023-02-27-2023-02-27_07-39-23Ognjen_ColovicSitzen\", 2)[0]\n",
    "    median4 = model.predict_classes('01_iPhone13ProMax-2023-03-15_18-29-42Gabriel_TorresRennen', 1)[0]\n",
    "    median5 = model.predict_classes(\"03_Huawei_Stehen-2023-03-16_15-27-54Lea_BuetlerStehen\", 3)[0]\n",
    "    median6 = model.predict_classes(\"01_iPhone14Pro_2023-02-28_20-09-53Yvo_KellerTreppenlaufen\", 4)[0]\n",
    "\n",
    "    correctness = [\n",
    "        int(median1 == 5.0),\n",
    "        int(median2 == 0.0),\n",
    "        int(median3 == 2.0),\n",
    "        int(median4 == 1.0),\n",
    "        int(median5 == 3.0),\n",
    "        int(median6 == 4.0)\n",
    "    ]\n",
    "\n",
    "    total_correct = sum(correctness)\n",
    "    print(f\"Total correct: {total_correct}\")\n",
    "\n",
    "# Check if the current model has the most correct predictions\n",
    "    if total_correct > most_correct:\n",
    "        most_correct = total_correct\n",
    "        best_model = model_name\n",
    "\n",
    "print(\"Best model:\", best_model)\n",
    "import os\n",
    "\n",
    "best_model_name = os.path.basename(best_model)\n",
    "best_model_path = os.path.join(\"saved_model\", \"sensor_model\", best_model_name)\n",
    "best_model = tf.keras.models.load_model(best_model_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss und Accuracy Verlauf vom besten Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model.name)\n",
    "\n",
    "# Create 2 subplots one for model accuracy and another for model loss\n",
    "fig, axs = plt.subplots(2, figsize=(15, 10))\n",
    "# Summarize history for accuracy\n",
    "axs[0].plot(best_model_history.history[\"accuracy\"])\n",
    "axs[0].plot(best_model_history.history[\"val_accuracy\"])\n",
    "axs[0].set_title(\"Model Accuracy from best model\")\n",
    "axs[0].set_ylabel(\"Accuracy\")\n",
    "axs[0].set_xlabel(\"Epoch\")  \n",
    "axs[0].legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "# Summarize history for loss\n",
    "axs[1].plot(best_model_history.history[\"loss\"])\n",
    "axs[1].plot(best_model_history.history[\"val_loss\"])\n",
    "axs[1].set_title(\"Model Loss\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].legend([\"train\", \"validation\"], loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusions Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create for x_train and x_test confusion matrix\n",
    "y_pred = best_model.predict(x_validation)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_test_labels = np.argmax(y_validation, axis=1)\n",
    "accuracy = accuracy_score(y_true = y_test_labels, y_pred = y_pred_labels)\n",
    "\n",
    "cm = confusion_matrix(y_true = y_test_labels, y_pred = y_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "\n",
    "# Create subplot for test confusion matrix\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7.5))\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax1, colorbar=False)\n",
    "ax1.set_title(\"Validation Confusion Matrix \\n \\\n",
    "                Accuracy: {:.2f}%, Datapoints: {}\".format(accuracy * 100, x_validation.shape[0]), fontsize=18)\n",
    "ax1.set_xticklabels(le.classes_, rotation=45)\n",
    "\n",
    "# Create for x_train and x_test confusion matrix\n",
    "y_pred = best_model.predict(x_train)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_true = y_train_labels, y_pred = y_pred_labels)\n",
    "cm = confusion_matrix(y_true = y_train_labels, y_pred = y_pred_labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "\n",
    "# Create subplot for train confusion matrix\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax2, colorbar=False)\n",
    "# add to title number of observations\n",
    "ax2.set_title(\"Train Confusion Matrix \\n \\\n",
    "              Accuracy: {:.2f}%, Datapoints: {}\".format(accuracy * 100, x_train.shape[0]), fontsize=18)\n",
    "ax2.set_xticklabels(le.classes_, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export bestes DL-Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export Model to json Tensorflow file for JS Frontend\n",
    "import json\n",
    "\n",
    "best_model.save(\"saved_model/sensor_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = \"saved_model/sensor_model/*.h5\"\n",
    "h5_files = glob.glob(path)\n",
    "print(h5_files)\n",
    "\n",
    "for h5_file in h5_files:\n",
    "    # make a confusion matrix for each model\n",
    "    model_name = os.path.basename(h5_file)  # Extract the file name from the path\n",
    "    print(f\"Model name: {model_name}\")\n",
    "    \n",
    "    # Load the model \n",
    "    model = tf.keras.models.load_model(h5_file)\n",
    "    \n",
    "    # Rest of your code...\n",
    "    # confusion matrix for each model with train and test data\n",
    "    # Create confusion matrix for train data x_train, x_validation, y_train, y_validation\n",
    "    \n",
    "    # Assuming you have defined the x_train, y_train, x_validation, and y_validation datasets\n",
    "    \n",
    "    train_generator = DataGenerator(x_train, y_train, Parameters.batch_size)\n",
    "    validation_generator = DataGenerator(x_validation, y_validation, Parameters.batch_size)\n",
    "    \n",
    "    y_pred = model.predict(train_generator)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    y_train_labels = np.argmax(y_train, axis=1)\n",
    "    accuracy = accuracy_score(y_true=y_train_labels, y_pred=y_pred_labels)\n",
    "    cm = confusion_matrix(y_true=y_train_labels, y_pred=y_pred_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "    # Create subplot for train confusion matrix\n",
    "    fig, ax2 = plt.subplots(figsize=(8, 6))\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax2, colorbar=False)\n",
    "    # add to title number of observations\n",
    "    ax2.set_title(\"Train Confusion {} Matrix \\n \\\n",
    "                Accuracy: {:.2f}%, Datapoints: {}\".format(model_name, accuracy * 100, x_train.shape[0]), fontsize=18)\n",
    "    ax2.set_xticklabels(le.classes_, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    y_pred = model.predict(validation_generator)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    y_validation_labels = np.argmax(y_validation, axis=1)\n",
    "    accuracy = accuracy_score(y_true=y_validation_labels, y_pred=y_pred_labels)\n",
    "    cm = confusion_matrix(y_true=y_validation_labels, y_pred=y_pred_labels)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "    # Create subplot for test confusion matrix\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 6))\n",
    "    disp.plot(cmap=plt.cm.Blues, ax=ax1, colorbar=False)\n",
    "    ax1.set_title(\"Validation Confusion Matrix \\n \\\n",
    "                Accuracy: {:.2f}%, Datapoints: {}\".format(accuracy * 100, x_validation.shape[0]), fontsize=18)\n",
    "    ax1.set_xticklabels(le.classes_, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
