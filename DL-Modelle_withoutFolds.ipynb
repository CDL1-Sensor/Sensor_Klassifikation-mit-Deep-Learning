{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensor Based Activity Recoginition \n",
    "Challenge: cdl1 - Sensor based Activity Recognition  \n",
    "Team: Lea Bütler, Manjavy Kirupa, Etienne Roulet, Si Ben Tran  \n",
    "\n",
    "Aufgabe: DL Modell erstellen\n",
    "\n",
    "Hier in diesem Notebook erstellen wir unsere Deep Learning Modelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import numpy as np\n",
    "\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "\n",
    "# datetime as filename for logging\n",
    "now = datetime.now()\n",
    "date_time_string = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, filename=f\"{date_time_string}.txt\", filemode=\"a\"\n",
    ")\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "# Static Parameters\n",
    "@dataclass\n",
    "class Parameters:\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 30\n",
    "    verbosity: str = \"auto\"\n",
    "    step_size: int = 374\n",
    "    number_folds: int = 6\n",
    "    output_size: int = 6\n",
    "    window_size = 300\n",
    "    step_size = 100\n",
    "    data_augmentation  = True\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Sensor_Data-Wrangling-und-EDA/Alle_Messungen_trimmed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time umwandeln "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the string time column to datetime\n",
    "epoch = pd.Timestamp(\"1970-01-01\")\n",
    "\n",
    "df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
    "df[\"time\"] = (df[\"time\"] - epoch).apply(lambda x: int(x.total_seconds() * 1000))\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select three random id_combines files\n",
    "ids = [\"06_iPhone12-2023-03-16_13-46-58Manjavy_KirupaVelofahren\", \"01_iPhone13pro-2023-03-21_16-55-47Etienne_RouletLaufen\", \"01_iPhone13ProMax-2023-03-15_18-29-42Gabriel_TorresRennen\"]\n",
    "df_validation = df[df[\"id_combined\"].isin(ids)]\n",
    "\n",
    "# export to csv \n",
    "df_validation.to_csv(\"validation-velo-laufen-rennen.csv\", index=False)\n",
    "\n",
    "# remove the validation data from the dataframe \n",
    "df = df[~df[\"id_combined\"].isin(ids)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unnötige Spalten und Class Encoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"id\", \"user\", \"id_combined\"])\n",
    "df.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "# get all types of the df\n",
    "le = LabelEncoder()\n",
    "df[\"class\"] = le.fit_transform(df[\"class\"])\n",
    "# print dictionary of the classes and its encoded values\n",
    "print(dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def add_noise(time_series, noise_factor):\n",
    "    noise = np.random.randn(len(time_series)) * noise_factor\n",
    "    return time_series + noise\n",
    "\n",
    "# Flip time series\n",
    "def flip_time_series(time_series):\n",
    "    return np.flip(time_series)\n",
    "\n",
    "# Scale magnitude\n",
    "def scale_magnitude(time_series, scaling_factor):\n",
    "    return time_series * scaling_factor\n",
    "\n",
    "# Augment the data\n",
    "def augment_data(X, y, noise_factor, scaling_factor, flip_probability):\n",
    "    augmented_X = []\n",
    "    augmented_y = []\n",
    "    for i in range(X.shape[0]):\n",
    "        x_sample = X[i]\n",
    "        y_sample = y[i]\n",
    "\n",
    "        # Add noise\n",
    "        x_sample_noisy = add_noise(x_sample, noise_factor)\n",
    "        augmented_X.append(x_sample_noisy)\n",
    "        augmented_y.append(y_sample)\n",
    "\n",
    "        # Scale magnitude\n",
    "        x_sample_scaled = scale_magnitude(x_sample, scaling_factor)\n",
    "        augmented_X.append(x_sample_scaled)\n",
    "        augmented_y.append(y_sample)\n",
    "\n",
    "        # Flip time series\n",
    "        if random.random() < flip_probability:\n",
    "            x_sample_flipped = flip_time_series(x_sample)\n",
    "            augmented_X.append(x_sample_flipped)\n",
    "            augmented_y.append(y_sample)\n",
    "\n",
    "    return np.array(augmented_X), np.array(augmented_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the window size and step size\n",
    "\n",
    "window_size = Parameters.window_size\n",
    "step_size = Parameters.step_size\n",
    "# Reshape X to 2D format (samples, features)\n",
    "X = df.values[:, 1:13]\n",
    "# Define y\n",
    "y = df[\"class\"].values\n",
    "X_windows = np.array(\n",
    "        [\n",
    "            X[i : i + window_size, :]\n",
    "            for i in range(0, X.shape[0] - window_size + 1, step_size)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Reshape X_windows to 3D format (samples, timesteps, features)\n",
    "timesteps = X_windows.shape[1]\n",
    "n_features = X_windows.shape[2]\n",
    "X_windows = X_windows.reshape(-1, timesteps, n_features)\n",
    "\n",
    "if (Parameters.data_augmentation == False):\n",
    "\n",
    "    # Create a sliding window of X with the specified window and step sizes\n",
    "    \n",
    "\n",
    "    # Create the corresponding y labels for the sliding windows\n",
    "    y_windows = np.array(\n",
    "        [y[i + window_size - 1] for i in range(0, X.shape[0] - window_size + 1, step_size)]\n",
    "    )\n",
    "    y_windows = to_categorical(y_windows, num_classes=6)\n",
    "\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_windows, y_windows, test_size=0.1, random_state=42, stratify=y_windows\n",
    "    )\n",
    "elif (Parameters.data_augmentation == True):\n",
    "        # Augmentation parameters\n",
    "    noise_factor = 0.1\n",
    "    scaling_factor = 1.5\n",
    "    flip_probability = 0.5\n",
    "    \n",
    "    # Augment the data\n",
    "    X_augmented, y_augmented = augment_data(X, y, noise_factor, scaling_factor, flip_probability)\n",
    "    \n",
    "    # Create sliding windows for the augmented data\n",
    "    X_windows_augmented = np.array(\n",
    "        [\n",
    "            X_augmented[i : i + window_size, :]\n",
    "            for i in range(0, X_augmented.shape[0] - window_size + 1, step_size)\n",
    "        ]\n",
    "    )\n",
    "    X_windows_augmented = X_windows_augmented.reshape(-1, timesteps, n_features)\n",
    "    \n",
    "    y_windows_augmented = np.array(\n",
    "        [y_augmented[i + window_size - 1] for i in range(0, X_augmented.shape[0] - window_size + 1, step_size)]\n",
    "    )\n",
    "    y_windows_augmented = to_categorical(y_windows_augmented, num_classes=6)\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_validation, y_train, y_validaton = train_test_split(\n",
    "        X_windows_augmented, y_windows_augmented, test_size=0.2, random_state=42, stratify=y_windows_augmented\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape, X_train.shape, y_train.shape, X_validation.shape, y_validaton.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Umschreibung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train\n",
    "x_validation = X_validation\n",
    "y_train = y_train\n",
    "y_validation = y_validaton"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL-Modelle erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Something like this as first Model\n",
    "\n",
    "def create_model_1(name=\"model_1\"):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=64,\n",
    "                kernel_size=12,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.LSTM(100),\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_2(name=\"model_2\"):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=32,\n",
    "                kernel_size=8,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300)),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def create_model_3(name=\"model_3\"):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=32,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.LSTM(100),\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_4(name=\"model_4\"):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=64,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            # Add LSTM layer\n",
    "            tf.keras.layers.LSTM(100),\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(\n",
    "                6, activation=\"softmax\"\n",
    "            ),  # Change activation function based on the nature of the output\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_5(name=\"model_5\"):\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            # Add a 1D convolutional layer\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=32,\n",
    "                kernel_size=2,\n",
    "                activation=\"relu\",\n",
    "                padding=\"same\",\n",
    "                input_shape=(timesteps, n_features),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=20, kernel_size=2, activation=\"relu\", padding=\"same\",\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=10, kernel_size=2, activation=\"relu\", padding=\"same\", kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "            ),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            # flatten output\n",
    "            tf.keras.layers.Flatten(),\n",
    "            # Add a dense output layer\n",
    "            tf.keras.layers.Dense(180, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dense(100, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.01),),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            # Change activation function based on the nature of the output\n",
    "            tf.keras.layers.Dense(6, activation=\"softmax\"),\n",
    "        ],\n",
    "        name=name,\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL-Modelle Trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_history = None  # Keep track of the best model's history\n",
    "model_histories = []\n",
    "\n",
    "# Perform training and validation\n",
    "models = [create_model_2]\n",
    "best_model = None\n",
    "best_model_acc = 0\n",
    "\n",
    "for j, model_creator in enumerate(models):\n",
    "    print(f\"Model {model_creator.__name__}\")\n",
    "    model_name = f\"Model_{j+1}\"\n",
    "    model = model_creator(name=model_name)\n",
    "    logging.info(f\"Model {j+1}\")\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=Parameters.epochs,\n",
    "        batch_size=Parameters.batch_size,\n",
    "        validation_data=(x_validation, y_validation),\n",
    "        verbose=Parameters.verbosity,\n",
    "    )\n",
    "    test_loss, acc, prec, recal = model.evaluate(\n",
    "        x_validation, y_validation, verbose=Parameters.verbosity\n",
    "    )\n",
    "    logging.info(f\"Validation accuracy: {acc}\")\n",
    "\n",
    "    model_histories.append(history.history)\n",
    "\n",
    "    for epoch in range(Parameters.epochs):\n",
    "        # Log accuracy after each epoch\n",
    "        acc_epoch = history.history[\"val_accuracy\"][epoch]\n",
    "        logging.info(f\"Epoch {epoch + 1}, Validation accuracy: {acc_epoch}\")\n",
    "\n",
    "    if best_model_history is None or acc > best_model_acc:\n",
    "        best_model_history = history\n",
    "        best_model = model  # Store the trained model instance\n",
    "        best_model_acc = acc\n",
    "\n",
    "print(best_model.name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss und Accuracy Verlauf vom besten Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model.name)\n",
    "display(best_model_history.history[\"accuracy\"])\n",
    "print(best_model_history.history[\"loss\"])\n",
    "# summarize history for accuracy\n",
    "plt.plot(best_model_history.history[\"accuracy\"])\n",
    "plt.plot(best_model_history.history[\"val_accuracy\"])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(best_model_history.history[\"loss\"])\n",
    "plt.plot(best_model_history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusions Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for multiclass classification\n",
    "y_pred = best_model.predict(x_validation)\n",
    "y_test_labels = y_validaton.argmax(axis=1)\n",
    "y_pred_labels = y_pred.argmax(axis=1)\n",
    "\n",
    "# create cm\n",
    "cm = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "display(le.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "# get f1 score of each class\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(y_test_labels, y_pred_labels, average=None)\n",
    "# get f1 score of each class\n",
    "\n",
    "# plot confusion matrix\n",
    "disp.plot()\n",
    "plt.show()\n",
    "display(f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Daten Predicten"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export bestes DL-Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export Model to json Tensorflow file\n",
    "import json\n",
    "\n",
    "best_model.save(\"saved_model/sensor_model.h5\")\n",
    "\n",
    "model = tf.keras.models.load_model(\"saved_model/sensor_model.h5\")\n",
    "\n",
    "# Save model architecture to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save model weights to a JSON file\n",
    "weights = model.get_weights()\n",
    "weights_as_list = [w.tolist() for w in weights]\n",
    "with open(\"weights.json\", \"w\") as f:\n",
    "    json.dump(weights_as_list, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell Einlesen und predicten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class validate_unseen_data():\n",
    "    def __init__(self, model_path=\"saved_model/sensor_model.h5\"):\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        self.csv_path = \"validation-velo-laufen-rennen.csv\"\n",
    "        \n",
    "    def predict_classes(self, file=\"06_iPhone12-2023-03-16_13-46-58Manjavy_KirupaVelofahren\", expected=''):\n",
    "        df_val = pd.read_csv(self.csv_path)\n",
    "        # Filter the rows where its velo in id_combined\n",
    "        df_val = df_val[df_val[\"id_combined\"].str.contains(file)]\n",
    "        df_val = df_val.drop(columns=[\"id\", \"user\", \"id_combined\"])\n",
    "        # convert the string time column to datetime\n",
    "        epoch = pd.Timestamp(\"1970-01-01\")\n",
    "        df_val[\"time\"] = pd.to_datetime(df_val[\"time\"])\n",
    "        df_val[\"time\"] = (df_val[\"time\"] - epoch).apply(\n",
    "            lambda x: int(x.total_seconds() * 1000)\n",
    "        )\n",
    "        df_val.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "        # get all types of the df\n",
    "        le = LabelEncoder()\n",
    "        df_val[\"class\"] = le.fit_transform(df_val[\"class\"])\n",
    "\n",
    "        # Set the window size and step size\n",
    "        window_size = 300\n",
    "        step_size = 100\n",
    "\n",
    "        # Reshape X to 2D format (samples, features)\n",
    "        X = df_val.values[:, 1:13]\n",
    "\n",
    "        # Create a sliding window of X with the specified window and step sizes\n",
    "        X_windows = np.array(\n",
    "            [\n",
    "                X[i : i + window_size, :]\n",
    "                for i in range(0, X.shape[0] - window_size + 1, step_size)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Reshape X_windows to 3D format (samples, timesteps, features)\n",
    "        timesteps = X_windows.shape[1]\n",
    "        n_features = X_windows.shape[2]\n",
    "        X_windows = X_windows.reshape(-1, timesteps, n_features)\n",
    "\n",
    "        y_pred_probs = model.predict(X_windows)\n",
    "\n",
    "        # Get the predicted class labels for each input window\n",
    "        y_pred_labels = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "        # Print the predicted class labels\n",
    "        # get the median of the predicted labels\n",
    "        sol = np.median(y_pred_labels)\n",
    "\n",
    "        class_counts = np.bincount(y_pred_labels)\n",
    "        for i, count in enumerate(class_counts):\n",
    "            print(f\"Class {i} count: {count}\")\n",
    "        \n",
    "        return (sol, expected)\n",
    "\n",
    "median = validate_unseen_data().predict_classes('06_iPhone12-2023-03-16_13-46-58Manjavy_KirupaVelofahren', expected='velo')\n",
    "median2 = validate_unseen_data().predict_classes('01_iPhone13ProMax-2023-03-15_18-29-42Gabriel_TorresRennen', expected='rennen')\n",
    "median3 = validate_unseen_data().predict_classes('01_iPhone13pro-2023-03-21_16-55-47Etienne_RouletLaufen', expected='laufen')\n",
    "\n",
    "(median, median2, median3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Conclusion is that the model is not overfitted but Gabriel is more a Stepper than a Runner ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
